{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67dbaff3386c8d3b",
   "metadata": {},
   "source": [
    "## Neural network with 2 hidden layers for image classification (digit-recognition):\n",
    "- using Relu as activation function\n",
    "- using Softmax"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T12:04:06.336427Z",
     "start_time": "2025-03-27T12:04:06.332731Z"
    }
   },
   "source": [
    "from typing import Dict, Tuple\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  Load MNIST Data from IDX Files\n",
    "We load the handwritten digit images and labels from the raw IDX format using `idx2numpy`. The training set contains 60,000 images of size 28×28, and the test set contains 10,000."
   ],
   "id": "2bf826ebcd5b5894"
  },
  {
   "cell_type": "code",
   "id": "e3754c0350824cc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T12:08:51.385769Z",
     "start_time": "2025-03-27T12:08:51.366396Z"
    }
   },
   "source": [
    "import idx2numpy\n",
    "\n",
    "# Training data\n",
    "X_train = idx2numpy.convert_from_file(\"archive/train-images.idx3-ubyte\")\n",
    "y_train = idx2numpy.convert_from_file(\"archive/train-labels.idx1-ubyte\")\n",
    "\n",
    "# Test data\n",
    "X_test = idx2numpy.convert_from_file(\"archive/t10k-images.idx3-ubyte\")\n",
    "y_test = idx2numpy.convert_from_file(\"archive/t10k-labels.idx1-ubyte\")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)  # Should be (60000, 28, 28)\n",
    "print(\"y_train shape:\", y_train.shape)  # Should be (60000,)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28)\n",
      "y_train shape: (60000,)\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  Flatten and Normalize Data\n",
    "We flatten each 28×28 image to a 784-length vector, normalize pixel values to [0, 1], and transpose the dataset so each column corresponds to one image sample."
   ],
   "id": "eedbd9981a1dc67d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T12:04:07.235354Z",
     "start_time": "2025-03-27T12:04:06.899435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We want to flatten the shape (784, num_batches or samples) to [1,0]\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1).T / 255.0 # shape: (784, 60000)\n",
    "X_test_flat =  X_test.reshape(X_test.shape[0], -1).T / 255.0 # shape: (784, 10000)\n",
    "\n",
    "print(f\"X_train_flat shape:{X_train_flat.shape}\")\n",
    "print(f\"X_test_flat shape:{X_test_flat.shape}\")\n",
    "print(X_train.shape[0])\n"
   ],
   "id": "d7ebb07b940a86b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_flat shape:(784, 60000)\n",
      "X_test_flat shape:(784, 10000)\n",
      "60000\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T12:04:07.242135Z",
     "start_time": "2025-03-27T12:04:07.237454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def initialze_parameters()-> Dict[str, np.ndarray]:\n",
    "    np.random.seed(42)\n",
    "    parameters: Dict[str: np.ndarray] = {\"W1\" : np.random.rand(128,784) * np.sqrt(2/784), # formula for He init => sqrt(2/m) with m is the number of inputs to the layer\n",
    "    \"b1\" : np.zeros((128,1)),\n",
    "    \"W2\" : np.random.rand(64,128) * np.sqrt(2/128),\n",
    "    \"b2\" : np.zeros((64, 1)),\n",
    "    \"W3\" : np.random.rand(10,64) * np.sqrt(2/64),\n",
    "    \"b3\" : np.zeros((10,1))\n",
    "    }\n",
    "    return parameters\n",
    "    "
   ],
   "id": "8b89cd8a2260aad7",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T12:04:07.247398Z",
     "start_time": "2025-03-27T12:04:07.243634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define the activation functions (ReLu + Softmax)\n",
    "def relu(Z: np.ndarray)-> np.ndarray:\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def softmax(Z: np.ndarray)-> np.ndarray:\n",
    "    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True)) # prevent overflow\n",
    "    return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
    "    "
   ],
   "id": "378348c597335b6f",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T12:04:07.349156Z",
     "start_time": "2025-03-27T12:04:07.344365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# forward_pass\n",
    "def forward_pass(X: np.ndarray, parameters: Dict[str, np.ndarray])-> Tuple[np.ndarray, Tuple]:\n",
    "\n",
    "    W1, b1 = parameters[\"W1\"], parameters[\"b1\"]\n",
    "    W2, b2 = parameters[\"W2\"], parameters[\"b2\"]\n",
    "    W3, b3 = parameters[\"W3\"], parameters[\"b3\"]\n",
    "    \n",
    "    Z1 = W1 @ X + b1\n",
    "    A1 = relu(Z1)\n",
    "    \n",
    "    Z2 = W2 @ A1 + b2\n",
    "    A2 = relu(Z2)\n",
    "    \n",
    "    Z3 = W3 @ A2 + b3\n",
    "    A3 = softmax(Z3)\n",
    "    \n",
    "    cache = (Z1, A1, Z2, A2, Z3, A3)\n",
    "    \n",
    "    return A3, cache"
   ],
   "id": "9a99a79c748a7e1",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  Evaluate Accuracy on First Batch\n",
    "We compute predictions using `argmax` on the softmax output and compare them to the true labels to calculate accuracy for the first mini-batch."
   ],
   "id": "13225c4e746f477b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T12:06:13.757726Z",
     "start_time": "2025-03-27T12:06:13.751446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test the forward pass on one batch\n",
    "\n",
    "# initalize the parameters\n",
    "parameters = initialze_parameters()\n",
    "\n",
    "# get some batches = 32\n",
    "X_batch = X_test_flat[:, :32]\n",
    "true_labels = y_train[:32]\n",
    "\n",
    "#Forward_pass\n",
    "A3, cache = forward_pass(X_batch, parameters)\n",
    "\n",
    "#Evaluation \n",
    "prediction = np.argmax(A3, axis=0)\n",
    "accuracy = np.mean(prediction == true_labels)\n",
    "\n",
    "print(\"Output probabilities shape:\", A3.shape)  # (10, 32)\n",
    "print(f\"First prediction :{np.round(A3[:, 0], 3)}\")\n",
    "print(f\"Accuracy on first batch: {accuracy * 100:.2f}%\")\n"
   ],
   "id": "aa7ba8545f970227",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output probabilities shape: (10, 32)\n",
      "First prediction :[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Accuracy on first batch: 6.25%\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Loss Function(Cross-Entropy)\n",
    "- I have chosen Cross-Entropy for my loss function becasue Softmax plus Cross entropy is easier to compute from scratch"
   ],
   "id": "a4003d22a26c2458"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T10:54:15.249406Z",
     "start_time": "2025-03-27T10:54:15.244664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_loss(A3: np.ndarray, Y: np.ndarray)-> float:\n",
    "    \"\"\"\n",
    "    Cross-entropy loss:\n",
    "    - A3: predicted probabilities (10, m)\n",
    "    - Y: one-hot true labels (10, m)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    loss = -np.sum(Y * np.log(A3 + 1e-8)) / m\n",
    "    return loss\n",
    "    "
   ],
   "id": "d7643044f70caf5c",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# One-Hot Encode Labels\n",
    "- Before backprop, we make sure y_train in one-hot encoded()"
   ],
   "id": "5bdcd6e850da1a84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T11:55:22.215178Z",
     "start_time": "2025-03-27T11:55:22.211408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def one_hot_encode(y :np.ndarray, num_classes: int = 10)-> np.ndarray:\n",
    "    one_hot= np.zeros(num_classes, y.size)\n",
    "    one_hot[y, np.arange(y.size)] = 1\n",
    "    return one_hot\n"
   ],
   "id": "6b50ce67086e7a54",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8daf2514e3d2b019"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T11:55:23.289644Z",
     "start_time": "2025-03-27T11:55:23.278666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_pass(X: np.ndarray, Y: np.ndarray, cache: Tuple, parameters: Dict[str, np.ndarray])-> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Backprop through the 3-layer NN using cached forward values.\n",
    "    \"\"\"\n",
    "    W1, W2, W3 = parameters[\"W1\"], parameters[\"W2\"], parameters[\"W3\"]\n",
    "    Z1, A1, Z2, A2, Z3, A3 = cache\n",
    "    m = X.shape[1]\n",
    "\n",
    "    # Output layer gradient (Softmax + CrossEntropy simplifies)\n",
    "    dZ3 = A3 - Y  # (10, m)\n",
    "    dW3 = (1 / m) * dZ3 @ A2.T\n",
    "    db3 = (1 / m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "\n",
    "    # Hidden Layer 2\n",
    "    dA2 = W3.T @ dZ3\n",
    "    dZ2 = dA2 * (Z2 > 0)  # ReLU backward\n",
    "    dW2 = (1 / m) * dZ2 @ A1.T\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    # Hidden Layer 1\n",
    "    dA1 = W2.T @ dZ2\n",
    "    dZ1 = dA1 * (Z1 > 0)  # ReLU backward\n",
    "    dW1 = (1 / m) * dZ1 @ X.T\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\n",
    "        \"dW1\": dW1, \"db1\": db1,\n",
    "        \"dW2\": dW2, \"db2\": db2,\n",
    "        \"dW3\": dW3, \"db3\": db3\n",
    "    }\n",
    "\n",
    "    return grads\n"
   ],
   "id": "42539c91d9968cd9",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T11:55:27.750567Z",
     "start_time": "2025-03-27T11:55:27.746057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_parameters(parameters: Dict[str, np.ndarray], grads: Dict[str, np.ndarray], learning_rate: float=0.1)-> Dict[str, np.ndarray]:\n",
    "    for key in parameters:\n",
    "        parameters[key] -= learning_rate * grads[\"d\" + key]\n",
    "    return parameters\n"
   ],
   "id": "d713d0edae575017",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:55:04.824523Z",
     "start_time": "2025-03-26T11:55:04.783181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "\n",
    "parameters = initialze_parameters()\n",
    "m = X_train_flat.shape[1]  # total training samples\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle the data at the start of each epoch\n",
    "    indices = np.random.permutation(m)\n",
    "    X_shuffled = X_train_flat[:, indices]\n",
    "    y_shuffled = y_train[indices]\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    batches = m // batch_size\n",
    "\n",
    "    for i in range(0, m, batch_size):\n",
    "        # Get mini-batch\n",
    "        X_batch = X_shuffled[:, i:i+batch_size]\n",
    "        y_batch = y_shuffled[i:i+batch_size]\n",
    "        Y_batch = one_hot_encode(y_batch)\n",
    "\n",
    "        # Forward pass\n",
    "        A3, cache = forward_pass(X_batch, parameters)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = compute_loss(A3, Y_batch)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Accuracy\n",
    "        predictions = np.argmax(A3, axis=0)\n",
    "        correct += np.sum(predictions == y_batch)\n",
    "\n",
    "        # Backward pass\n",
    "        grads = backward_pass(X_batch, Y_batch, cache, parameters)\n",
    "\n",
    "        # Update weights\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "    # Epoch summary\n",
    "    epoch_loss = total_loss / batches\n",
    "    epoch_accuracy = correct / m * 100\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.4f} | Accuracy: {epoch_accuracy:.2f}%\")\n"
   ],
   "id": "1fd12514fedd5e4c",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "56ceb02db359d7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T18:41:54.942416Z",
     "start_time": "2025-03-26T18:41:54.930219Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d66abea1ec1c0990",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "55029236782164e7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
