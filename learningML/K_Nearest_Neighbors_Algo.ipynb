{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b87451",
   "metadata": {},
   "source": [
    "# Understanding the K-Nearest Neighbors (KNN) Algorithm\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a simple, yet powerful machine learning algorithm that is used for both classification and regression. However, it is more widely used in classification problems in the industry. In this notebook, we will cover:\n",
    "\n",
    "- Why KNN is a necessary algorithm in a data scientist's toolbox.\n",
    "- The fundamentals of the KNN algorithm.\n",
    "- How to choose the appropriate number of neighbors 'K'.\n",
    "- Situations in which KNN is a suitable model choice.\n",
    "- The mechanism of the KNN algorithm.\n",
    "- A practical use case: Predicting diabetes using KNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed2e9f4",
   "metadata": {},
   "source": [
    "## Why Do We Need KNN?\n",
    "\n",
    "KNN is valuable for several reasons:\n",
    "\n",
    "- **Ease of understanding and implementation**: The logic behind KNN is simple to grasp, making it an excellent starting point for practitioners to start exploring machine learning models.\n",
    "- **No assumption about data**: KNN is a non-parametric algorithm, which means it does not make any assumptions about the underlying data distribution. This is helpful with real-world data that often does not follow theoretical assumptions.\n",
    "- **Versatility**: It can be used for classification, regression, and search (as in recommender systems).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28390ff",
   "metadata": {},
   "source": [
    "## What is KNN?\n",
    "\n",
    "KNN works by finding the nearest data points in the training set to the new point that you want to classify. The 'K' in KNN is the number of nearest neighbors you want to consider. The category of the majority of the K nearest neighbors is the category into which the new data point is classified.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a96a0f0",
   "metadata": {},
   "source": [
    "## How Do We Choose the Factor 'K'?\n",
    "\n",
    "Choosing the right value for 'K' is critical to the performance of the algorithm. A smaller 'K' value can make the algorithm sensitive to noise in the data, while a larger 'K' value can smooth out the prediction but might blur the boundaries between classes. Cross-validation is typically used to select an optimal 'K' value that provides a balance between bias and variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed9de6",
   "metadata": {},
   "source": [
    "## When Do We Use KNN?\n",
    "\n",
    "KNN can be a good choice when:\n",
    "\n",
    "- You have a labeled dataset.\n",
    "- The dataset is not too large, as KNN can be slow to predict with many data points.\n",
    "- The problem is a classification problem, though KNN can be used for regression as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eae741a",
   "metadata": {},
   "source": [
    "## How Does KNN Algorithm Work?\n",
    "\n",
    "When a new data point is introduced, the algorithm looks through the entire training set for the K most similar instances (the neighbors) and assigns the new data point to the class most common among its K nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a4a6c6",
   "metadata": {},
   "source": [
    "## Use Case: Predicting Diabetes\n",
    "\n",
    "Let's explore a practical example where we apply the KNN algorithm to predict whether a person will have diabetes based on certain diagnostics measurements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4a5116",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) for Diabetes Prediction\n",
    "\n",
    "In this example, we'll use the K-Nearest Neighbors algorithm to build a model that can predict whether a person has diabetes based on diagnostic measurements. This is a binary classification problem where we will predict one of two possible outcomes: diabetic or not diabetic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c6f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f686f7a",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We'll start by loading the Pima Indians Diabetes dataset, which includes various diagnostic measures that can be used to predict diabetes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31e6ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('learningML/diabetes.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cc7c1b",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "It's essential to explore the data to understand the distribution and check for any inconsistencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd0d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d7a576",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We'll preprocess the data by scaling the features since KNN performs better with normalized data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f758253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features from the target attribute\n",
    "X = df.drop('Outcome', axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c489d",
   "metadata": {},
   "source": [
    "## Split the Dataset\n",
    "\n",
    "Next, we split the dataset into a training set and a test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ef15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b8bb6f",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "We'll find the optimal value of K by using cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a077a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# List to hold the cross-validation scores\n",
    "cv_scores = []\n",
    "\n",
    "# Try different values for K\n",
    "for k in range(1, 21):\n",
    "    knn = KNeighborsClassifier(n=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# Find the optimal K\n",
    "optimal_k = np.argmax(cv_scores) + 1\n",
    "print(f'The optimal number of neighbors is {optimal_k}')\n",
    "\n",
    "# Plot the CV accuracy for each K\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 21), cv_scores, marker='o', linestyle='dashed')\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('CV Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6527933",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "We'll evaluate the model on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e697a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the KNN classifier with the optimal K\n",
    "knn = KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
