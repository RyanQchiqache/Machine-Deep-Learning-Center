{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6053453d",
   "metadata": {},
   "source": [
    "# Exercise 8: Self Supervised Learning\n",
    "\n",
    "**Summer Semester 2025**\n",
    "\n",
    "**Author**: Nick Stracke (nick.stracke@lmu.de)\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Parts where code should be added are marked using `TODO`. You shouldn't have to edit anything else with the exception of imports.\n",
    "\n",
    "*Please* add extensive comments to your code to explain what you're doing/trying to do. It is hard to assign partial points for incorrect or incomplete solutions if we do not understand what you are trying to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d91ba",
   "metadata": {},
   "source": [
    "# Task 1: Temperature in Self Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "id": "62c59802",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T15:36:50.890718Z",
     "start_time": "2025-07-10T15:36:50.885987Z"
    }
   },
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "a491fe5b",
   "metadata": {},
   "source": [
    "### Task 1.1: Visualizing the Effect of Temperature"
   ]
  },
  {
   "cell_type": "code",
   "id": "f65ae9a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T15:36:52.146792Z",
     "start_time": "2025-07-10T15:36:52.139495Z"
    }
   },
   "source": [
    "def softmax_temp(logits: torch.Tensor, tau: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies the softmax function with temperature.\n",
    "\n",
    "    Parameters:\n",
    "        logits (torch.Tensor, shape (N,)): A tensor containing the raw scores or logits.\n",
    "        tau (float, optional): The temperature parameter to scale the logits. Default is 1.0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the probabilities after applying the softmax function.\n",
    "    \"\"\"\n",
    "    scaled_logits = logits/tau\n",
    "    max_logits = torch.max(scaled_logits, dim=-1, keepdim=True).values\n",
    "    stabilized_logits = scaled_logits - max_logits\n",
    "    \n",
    "    exp_logits = torch.exp(stabilized_logits)\n",
    "    \n",
    "    sum_exp = torch.sum(exp_logits, dim=-1, keepdim=True)\n",
    "    softmax = exp_logits / sum_exp\n",
    "    \n",
    "    \n",
    "    return softmax"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "8a21355b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T15:36:55.025Z",
     "start_time": "2025-07-10T15:36:53.965169Z"
    }
   },
   "source": [
    "sims = torch.tensor([2.0, 1.0, 0.2, -0.5])\n",
    "temps = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "fig, ax = plt.subplots(1, len(temps), figsize=(int(2 * len(temps)), 2))\n",
    "for i, tau in enumerate(temps):\n",
    "    ax[i].bar(range(len(sims)), softmax_temp(sims, tau).numpy())\n",
    "    ax[i].set_title(f\"tau={tau}\")\n",
    "    ax[i].set_xticks(range(len(sims)))\n",
    "    ax[i].set_xticklabels([f\"s{i}\" for i in range(len(sims))])\n",
    "    ax[i].set_ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "id": "fe9c113b",
   "metadata": {},
   "source": [
    "### Task 1.2: Intuition: Confidence vs. Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6b3769",
   "metadata": {},
   "source": [
    "##### What happens to the distribution when the temperature is very low (e.g., τ = 0.1)? What does this mean in terms of model confidence?\n",
    "\n",
    "*it makes the output more confident (peaky)*\n",
    "\n",
    "##### What happens when τ is very high (e.g., τ = 10.0)? What does this mean for the model’s ability to focus?\n",
    "\n",
    "*makes the output softer (more uniform) and can lead to nonsense*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c4dc27",
   "metadata": {},
   "source": [
    "### Task 1.3: Why Temperature Matters in Contrastive Learning"
   ]
  },
  {
   "cell_type": "code",
   "id": "67de1156",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T15:36:56.894750Z",
     "start_time": "2025-07-10T15:36:56.538994Z"
    }
   },
   "source": [
    "sims = torch.tensor([1.0, 0.2, 0.1, -0.1, -0.3])\n",
    "temps = [0.1, 10.0]\n",
    "\n",
    "fig, ax = plt.subplots(1, len(temps), figsize=(int(2 * len(temps)), 2))\n",
    "for i, tau in enumerate(temps):\n",
    "    ax[i].bar(range(len(sims)), softmax_temp(sims, tau).numpy())\n",
    "    ax[i].set_title(f\"tau={tau}\")\n",
    "    ax[i].set_xticks(range(len(sims)))\n",
    "    ax[i].set_xticklabels([f\"s{i}\" for i in range(len(sims))])\n",
    "    ax[i].set_ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 400x200 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "id": "a65909b8",
   "metadata": {},
   "source": [
    "##### Discuss: Why might a sharper distribution (lower τ ) help the model learn more effectively?\n",
    "\n",
    "*Because a lower τ makes the model more confident in distinguishing positive (TP) pairs from negative (TN)\n",
    " pairs by amplifying the differences in similarity scores.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b148f4d",
   "metadata": {},
   "source": [
    "# Task 2: SimCLR"
   ]
  },
  {
   "cell_type": "code",
   "id": "46686779",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T15:11:40.149410Z",
     "start_time": "2025-07-10T15:11:40.145679Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "from ex8_2 import SimCLRModel\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "08668bd2",
   "metadata": {},
   "source": [
    "### Task 2.1: Implement and Train a SimCLR Model\n",
    "##### Switch to ex2.py to complete the implementation and train the SimCLR model. Come back here once you have a trained checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43eb60d",
   "metadata": {},
   "source": [
    "### Task 2.2: Visualize with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "id": "a51f0111",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T15:28:03.579744Z",
     "start_time": "2025-07-10T15:28:03.477823Z"
    }
   },
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_batch_size = 256\n",
    "model = SimCLRModel()\n",
    "model.load_state_dict(torch.load(\"simclr_model.pth\", map_location=\"cpu\"))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=T.ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, train_labels in tqdm(test_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        embedings = model.encoder(imgs)\n",
    "        all_embeddings.append((embedings.cpu()))\n",
    "        all_labels.append(train_labels)\n",
    "\n",
    "all_embeddings = torch.cat(all_embeddings, dim=0).squeeze().numpy()\n",
    "all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "subset_embeddings = all_embeddings[:1000]  # Subset for faster t-SNE\n",
    "subset_labels = all_labels[:1000]\n",
    "\n",
    "print(\"Running t-SNE...\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, init='random', random_state=42)\n",
    "tsne_result = tsne.fit_transform(subset_embeddings)\n",
    "\n",
    "# --- Plotting ---\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=subset_labels, cmap='tab10', s=10)\n",
    "plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.title(\"t-SNE of CIFAR-10 Test Set Embeddings\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'simclr_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[32]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      2\u001B[39m test_batch_size = \u001B[32m256\u001B[39m\n\u001B[32m      3\u001B[39m model = SimCLRModel()\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m model.load_state_dict(\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msimclr_model.pth\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcpu\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[32m      5\u001B[39m model.eval()\n\u001B[32m      6\u001B[39m model.to(device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/myproject-env-py3118/lib/python3.11/site-packages/torch/serialization.py:998\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[39m\n\u001B[32m    995\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m'\u001B[39m\u001B[33mencoding\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args.keys():\n\u001B[32m    996\u001B[39m     pickle_load_args[\u001B[33m'\u001B[39m\u001B[33mencoding\u001B[39m\u001B[33m'\u001B[39m] = \u001B[33m'\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m998\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[32m    999\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[32m   1000\u001B[39m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[32m   1001\u001B[39m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[32m   1002\u001B[39m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[32m   1003\u001B[39m         orig_position = opened_file.tell()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/myproject-env-py3118/lib/python3.11/site-packages/torch/serialization.py:445\u001B[39m, in \u001B[36m_open_file_like\u001B[39m\u001B[34m(name_or_buffer, mode)\u001B[39m\n\u001B[32m    443\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[32m    444\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[32m--> \u001B[39m\u001B[32m445\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    446\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    447\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m'\u001B[39m\u001B[33mw\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/myproject-env-py3118/lib/python3.11/site-packages/torch/serialization.py:426\u001B[39m, in \u001B[36m_open_file.__init__\u001B[39m\u001B[34m(self, name, mode)\u001B[39m\n\u001B[32m    425\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[32m--> \u001B[39m\u001B[32m426\u001B[39m     \u001B[38;5;28msuper\u001B[39m().\u001B[34m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'simclr_model.pth'"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "219525e1",
   "metadata": {},
   "source": [
    "### Task 2.3: Quantitative Evaluation using Linear Probing"
   ]
  },
  {
   "cell_type": "code",
   "id": "49137ecd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T15:28:04.921022Z",
     "start_time": "2025-07-10T15:28:04.865615Z"
    }
   },
   "source": [
    "# CONFIG\n",
    "num_classes = 10  # CIFAR-10 has 10 classes\n",
    "train_batch_size = 256\n",
    "epochs = 50\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=T.ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "train_features, train_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(train_loader):\n",
    "        x = x.to(device)\n",
    "        embedings_en = model.encoder(x)\n",
    "        all_embeddings.append(embedings_en.cpu())\n",
    "        all_labels.append(y)\n",
    "\n",
    "train_features = torch.cat(train_features)\n",
    "train_labels = torch.cat(train_labels)"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or corrupted. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[33]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      3\u001B[39m train_batch_size = \u001B[32m256\u001B[39m\n\u001B[32m      4\u001B[39m epochs = \u001B[32m50\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m train_dataset = \u001B[43mtorchvision\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdatasets\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCIFAR10\u001B[49m\u001B[43m(\u001B[49m\u001B[43mroot\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m./data\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m=\u001B[49m\u001B[43mT\u001B[49m\u001B[43m.\u001B[49m\u001B[43mToTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=\u001B[38;5;28;01mFalse\u001B[39;00m, num_workers=\u001B[32m0\u001B[39m)\n\u001B[32m      9\u001B[39m train_features, train_labels = [], []\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/myproject-env-py3118/lib/python3.11/site-packages/torchvision/datasets/cifar.py:68\u001B[39m, in \u001B[36mCIFAR10.__init__\u001B[39m\u001B[34m(self, root, train, transform, target_transform, download)\u001B[39m\n\u001B[32m     65\u001B[39m     \u001B[38;5;28mself\u001B[39m.download()\n\u001B[32m     67\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._check_integrity():\n\u001B[32m---> \u001B[39m\u001B[32m68\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mDataset not found or corrupted. You can use download=True to download it\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     70\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.train:\n\u001B[32m     71\u001B[39m     downloaded_list = \u001B[38;5;28mself\u001B[39m.train_list\n",
      "\u001B[31mRuntimeError\u001B[39m: Dataset not found or corrupted. You can use download=True to download it"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "ba22bf2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T15:28:05.724631Z",
     "start_time": "2025-07-10T15:28:05.685148Z"
    }
   },
   "source": [
    "# TODO: define the linear probe, optimizer and classification loss here\n",
    "probe = torch.nn.Linear(train_features.shape[1], num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(probe.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# Train probe\n",
    "for epoch in range(epochs):\n",
    "    perm = torch.randperm(train_features.size(0))\n",
    "    train_features, train_labels = train_features[perm], train_labels[perm]\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(train_features), train_batch_size):\n",
    "        xb = train_features[i:i+train_batch_size].to(device)\n",
    "        yb = train_labels[i:i+train_batch_size].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits= probe(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        \n",
    "    axg_loss = total_loss / len(train_features)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {axg_loss:.4f}\")"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[34]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# TODO: define the linear probe, optimizer and classification loss here\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m probe = torch.nn.Linear(\u001B[43mtrain_features\u001B[49m.shape[\u001B[32m1\u001B[39m], num_classes).to(device)\n\u001B[32m      3\u001B[39m optimizer = torch.optim.Adam(probe.parameters(), lr=\u001B[32m1e-3\u001B[39m)\n\u001B[32m      4\u001B[39m loss_fn = torch.nn.CrossEntropyLoss()\n",
      "\u001B[31mNameError\u001B[39m: name 'train_features' is not defined"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fab6fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        feats = model.encoder(x)\n",
    "        test_features.append(feats.cpu())\n",
    "        test_labels.append(y)\n",
    "\n",
    "test_features = torch.cat(test_features).to(device)\n",
    "test_labels = torch.cat(test_labels).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = probe(test_features)\n",
    "    preds = logits.argmax(dim=1)\n",
    "    accuracy = (preds == test_labels).float().mean().item()\n",
    "\n",
    "print(f\"Linear probe test accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfa53c8",
   "metadata": {},
   "source": [
    "# Task 3: CLIP for Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90980166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca17202",
   "metadata": {},
   "source": [
    "###  Task 3.2: Implement a Zero-Shot Classifier with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19089901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained CLIP\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model.eval()\n",
    "\n",
    "# Example images\n",
    "image_urls = [\n",
    "    \"https://images.unsplash.com/photo-1518717758536-85ae29035b6d\",  # dog\n",
    "    \"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba\",  # cat\n",
    "    # TODO: add some more images\n",
    "]\n",
    "\n",
    "images = [Image.open(requests.get(url, stream=True).raw).convert(\"RGB\") for url in image_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f046d988",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompts = [\n",
    "    \"a photo of a dog\",\n",
    "    \"an image of a cat\",\n",
    "    \"a photo of a animal\",\n",
    "    \"a photo of a horse\",\n",
    "    \"a photo of a car\",\n",
    "    # TODO: add some more text prompts\n",
    "    # try to play around with how you phrase the text prompts and how that affects the predictions\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc0e014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs for CLIP\n",
    "inputs = processor(text=text_prompts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "probs: torch.Tensor = None # shape [num_images, num_texts]\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # shape [num_images, num_texts]\n",
    "    probs = logits_per_image.softmax(dim=1)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112036cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print predictions\n",
    "for i, img in enumerate(images):\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"PREDICTED: {text_prompts[probs[i].argmax()]}\")\n",
    "    plt.show()\n",
    "    print(\"Probabilities:\")\n",
    "    for j, txt in enumerate(text_prompts):\n",
    "        print(f\"  {txt:<25}: {probs[i][j]:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b65aeb",
   "metadata": {},
   "source": [
    "### Task 3.3: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8523a908",
   "metadata": {},
   "source": [
    "##### Try varying your text prompts. Does the classification change?\n",
    "\n",
    "*your answer...*\n",
    "\n",
    "##### What kind of errors does CLIP make? Are they semantically reasonable?\n",
    "\n",
    "*your answer...*\n",
    "\n",
    "##### How does this compare to traditional classification models?\n",
    "\n",
    "*your answer...*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
