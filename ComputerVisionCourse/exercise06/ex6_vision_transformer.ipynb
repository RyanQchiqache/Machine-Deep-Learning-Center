{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7872eeb5",
   "metadata": {},
   "source": [
    "# Vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08d6823",
   "metadata": {},
   "source": [
    "Transformer is a family of neural network architectures that came to computer vision from NLP. Since transformers don't assume that their input has any specific structure, they can learn more general dependencies in data than convolutional neural network architectures. That's why we all like Vision transformers. At the same time, vision transformes are known to be \"data hungry\" and their training is quite tricky.\n",
    "\n",
    "In this homework we will go through main components of vision transformers and their training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d5684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb93c2",
   "metadata": {},
   "source": [
    "## How to code your transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e8ab73",
   "metadata": {},
   "source": [
    "As it was said, vision transformer came from NLP area where typical neural network input is ordered sequence of tokens which are words or word parts. So vision transformer main blocks are:\n",
    "1. Tokenizer - module that takes images and returns a set of tokens\n",
    "2. Transformer encoder - the main block of neural network that contains multihead attention, normalization and MLP on tokens.\n",
    "3. Positional embeddings - a way how to provide information about token orders\n",
    "4. Classification token - special token whose features is expected to be used for the final class prediction\n",
    "5. Classification head - MLP that predicts the final class from classificaiton token features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba2f4c",
   "metadata": {},
   "source": [
    "### Tokenizer (2p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799c007",
   "metadata": {},
   "source": [
    "Tokenizer should take an image, split it on non-overlapping patches, flatten the patches and apply Linear layer to these vectors. There are many ways how one can implement this, we will do it using Conv2D with stride being equal to kernel_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50abdc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(nn.Module):\n",
    "    def __init__(self, input_height, input_width, output_height, output_width,\n",
    "                 n_input_channels,\n",
    "                 embedding_dim):\n",
    "        super(Tokenizer, self).__init__()\n",
    "\n",
    "        assert input_height % output_height == 0, f\"{input_height} should be devided by {output_height}\"\n",
    "        assert input_width % output_width == 0, f\"{input_width} should be devided by {output_width}\"\n",
    "        \n",
    "        kernel_size = input_height // output_height\n",
    "        assert kernel_size == input_width // output_width\n",
    "\n",
    "        \n",
    "        self.conv = # YOUR CODE\n",
    "\n",
    "        self.flattener = nn.Flatten(2, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.flattener(self.conv(x)).transpose(-2, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf6e94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(input_height=32, input_width=32, output_height=16, output_width=16, n_input_channels=1,\n",
    "                      embedding_dim=64)\n",
    "dummy_batch = torch.zeros((1, 1, 32, 32))\n",
    "tokenizer_result = tokenizer.forward(dummy_batch)\n",
    "assert tokenizer_result.shape[1] == 16*16, tokenizer_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b7e5e",
   "metadata": {},
   "source": [
    "### Transformer encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981d258b",
   "metadata": {},
   "source": [
    "Transformer encoder consists of 2 blocks: Multi-Head Attention and MLP, each of each is prepended by layer norm. Let's walk through the separate modules for beggining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aee083",
   "metadata": {},
   "source": [
    "#### Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b99e7f",
   "metadata": {},
   "source": [
    "<img src=\"https://data-science-blog.com/wp-content/uploads/2022/01/mha_img_original.png\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eacf00",
   "metadata": {},
   "source": [
    "Attention implements a simple formula: $\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$.\n",
    "\n",
    "Multi-head attention is about splitting Q, K, V on several subvectors, appling Attention on each subvector independendly and concating the result.\n",
    "\n",
    "You can find Multi-Head Attention being implemented in pytorch as `torch.nn.MultiheadAttention`. Check the documentation and pay attention on `dropout` and `batch_first` parameters.\n",
    "\n",
    "[[paper]](https://arxiv.org/pdf/1706.03762.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.MultiheadAttention??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b57604",
   "metadata": {},
   "source": [
    "#### MLP for Transformer Encoder (1p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd93b81",
   "metadata": {},
   "source": [
    "MLP for transformer encoder is just a simple two-layer perceptron with RELU as non-linearity. It also uses Dropout after each Linear layer in order to reduce overfitting. Important thing is that size of hidden state on MLP is usually several times bigger than size of MLP input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845ef548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(embedding_dim, mlp_size, dropout_rate):\n",
    "    return nn.Sequential(\n",
    "        # YOUR CODE: Linear + RELU + Dropout + Linear + Dropout\n",
    "        nn.Linear(..\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94014c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = create_mlp(128, 128 * 2, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07328ab",
   "metadata": {},
   "source": [
    "#### Layer norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970438d",
   "metadata": {},
   "source": [
    "While Batch Normalization is a default normalization layer for convolutional neural networks, in transformers Layer Normalization is used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6766c262",
   "metadata": {},
   "source": [
    "Layer norm is implemented in pytorch as `torch.nn.LayerNorm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90723060",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.LayerNorm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295fdab4",
   "metadata": {},
   "source": [
    "#### TransformerEncoder: putting it all together (2p)\n",
    "\n",
    "Now we are ready to define Transformer Encoder.\n",
    "<img src=\"./transformer_encoder.png\" style=\"width:20%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf2fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, mlp_size, dropout=0.1, attention_dropout=0.1,\n",
    "                 drop_path_rate=0.1):\n",
    "        super().__init__()\n",
    "        # YOUR CODE\n",
    "        self.attention_pre_norm = ...\n",
    "        self.attention = torch.nn.MultiheadAttention(...)\n",
    "        self.attention_output_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mlp_pre_norm = ...\n",
    "        self.mlp = create_mlp(...)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # first block\n",
    "        y = self.attention_pre_norm(x)\n",
    "        attention = self.attention(y, y, y)[0]\n",
    "        attention = self.attention_output_dropout(attention)\n",
    "        x = x + attention   # Residual connection\n",
    "            \n",
    "        # second block\n",
    "        y = self.mlp_pre_norm(x)\n",
    "        y = self.mlp(y)\n",
    "        x = x + self.drop_path(y)  # Residual connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3d4acf",
   "metadata": {},
   "source": [
    "Let's check that it actually works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109eeb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c8d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = TransformerEncoder(embedding_dim=64, num_heads=2, mlp_size=128)\n",
    "encoder_result = e(tokenizer_result)\n",
    "print (encoder_result.shape)\n",
    "assert encoder_result.shape == tokenizer_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a16e020",
   "metadata": {},
   "source": [
    "### Positional embeddings (2p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec405abd",
   "metadata": {},
   "source": [
    "Positional embeddings is a way to give transformer information about token orders. You can either learn good embeddings by SGD or use some scheme for embeddings generation. The most popular scheme is sinusoidal embeddings:\n",
    "\n",
    "$$\\text{emb}(p, 2i) = \\sin(\\frac{p}{10000^{2i/d}})$$\n",
    "$$\\text{emb}(p, 2i + 1) = \\cos(\\frac{p}{10000^{2i/d}})$$\n",
    "where p, 2i, 2i+1 - indices of embedding element, d - embedding dimension\n",
    "\n",
    "Tranditional way of using embeddings in pytorch is by `torch.nn.Embedding`. But in our case its simplier to use more low-level thing `torch.nn.Parameter`. Here is how one can define learnable embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a607f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 256\n",
    "embedding_dim = 64\n",
    "\n",
    "# YOUR CODE\n",
    "emb =  torch.nn.Parameter(...)\n",
    "\n",
    "_ = torch.nn.init.trunc_normal_(emb, std=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1553d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emb.std(), emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9879388",
   "metadata": {},
   "source": [
    "### Class token and classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53167f24",
   "metadata": {},
   "source": [
    "Vanilla Vision Transformer uses a rather unusual way how to get the embedding of the whole image for the final prediction. It adds one more token, named as class-token, with its own positional embedding and takes its features as the final embedding of image. Alternative approach that comes from CNN is to use global average pooling for image embeddings obtaining. While being more simple to implement, global average pooling introduces a shortcut how different patches can communicates between each other (in vanilla ViT all the inter-patch relations can be learned only through attention blocks).\n",
    "\n",
    "However in modern papers you can meet the both approaces equally likely.\n",
    "\n",
    "Adding class token in pytorch is simple thing. You can either add one more embedding to `nn.Parameter` for positional encoders or create one more `nn.Parameter` module for class token only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "class_emb = torch.nn.Parameter(torch.empty((1, embedding_dim)), requires_grad=True)\n",
    "torch.nn.init.trunc_normal_(class_emb, std=0.2)\n",
    "\n",
    "print(class_emb[0].shape, class_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93121d04",
   "metadata": {},
   "source": [
    "### Vision Transformer: putting it all together (3p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaab8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_height, input_width,\n",
    "                 n_tokens,\n",
    "                 n_input_channels,\n",
    "                 embedding_dim,\n",
    "                 num_layers,\n",
    "                 num_heads,\n",
    "                 num_classes=1000,\n",
    "                 mlp_ratio=4.0,\n",
    "                 dropout=0.1,\n",
    "                 attention_dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # YOUR CODE\n",
    "        # 1. Tokenizer\n",
    "        self.tokenizer = Tokenizer(...)\n",
    "        \n",
    "        # 2. Positional embeddings\n",
    "        self.positional_embeddings = torch.nn.Parameter(...)\n",
    "        torch.nn.init.trunc_normal_(self.positional_embeddings, std=0.2)\n",
    "        \n",
    "        # 3. Class token\n",
    "        self.class_embedding = torch.nn.Parameter(torch.empty((1, embedding_dim)), requires_grad=True)\n",
    "        torch.nn.init.trunc_normal_(self.class_embedding, std=0.2)\n",
    "\n",
    "        # 4. TransformerEncoder \n",
    "        mlp_size = int(embedding_dim * mlp_ratio)\n",
    "        \n",
    "        \n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerEncoder(...)\n",
    "            for i in range(num_layers)])\n",
    "        \n",
    "        # 5. we will need more dropout and normalization!\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # 6. layer for the final prediction\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. tokenizer\n",
    "        patch_embeddings = ...\n",
    "        \n",
    "        # 2. position embeddings\n",
    "        patch_embeddings += self.positional_embeddings\n",
    "        \n",
    "        # 3. adding class token\n",
    "        cls_token = ...\n",
    "        \n",
    "        x = torch.cat(...)\n",
    "\n",
    "        # dropout!\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 4. transformer encoder blocks\n",
    "        for block in self.blocks:\n",
    "            x = ...\n",
    "            \n",
    "        # 5. final normalization\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # 6. final prediction from class-token embeddings\n",
    "        x = ...\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adde091",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = 16\n",
    "input_width = 16\n",
    "n_input_channels = 1\n",
    "vit = VisionTransformer(input_height, input_width,\n",
    "                 n_tokens=4,\n",
    "                 n_input_channels=n_input_channels,\n",
    "                 embedding_dim=32,\n",
    "                 num_layers=2,\n",
    "                 num_heads=2,\n",
    "                 num_classes=10,\n",
    "                 mlp_ratio=2.0,\n",
    "                 dropout=0.1,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69ac628",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_batch = torch.rand((1, n_input_channels, input_height, input_width))\n",
    "print(fake_batch.shape)\n",
    "\n",
    "result = vit(fake_batch)\n",
    "print(result.shape)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
