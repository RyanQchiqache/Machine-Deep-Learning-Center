{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSBPHdkKQFPf"
   },
   "source": [
    "<h1 align=\"center\"> Image Captioning With Attention</h1>\n",
    "\n",
    "In this notebook you will teach network to use attention during captioning images.\n",
    "\n",
    "Here is what we need to do:\n",
    "1. Take pretrained VGG19 to build feature vectors for positions of images.\n",
    "2. Stack LSTM with attention on top of that.\n",
    "3. Train the model, draw attention maps.\n",
    "\n",
    "This assignment is based on the paper \"Show, Attend and Tell\" (https://arxiv.org/abs/1502.03044)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nxkHHkNbQFPg",
    "ExecuteTime": {
     "end_time": "2025-07-04T12:12:52.592727Z",
     "start_time": "2025-07-04T12:12:52.584471Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "guMeC5h3QFPg",
    "ExecuteTime": {
     "end_time": "2025-07-04T12:12:52.606056Z",
     "start_time": "2025-07-04T12:12:52.602871Z"
    }
   },
   "source": [
    "import json"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QESXu_EkQFPg",
    "ExecuteTime": {
     "end_time": "2025-07-04T12:12:54.044008Z",
     "start_time": "2025-07-04T12:12:52.722123Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7lHZXKhsQFPh",
    "ExecuteTime": {
     "end_time": "2025-07-04T12:13:08.943986Z",
     "start_time": "2025-07-04T12:12:58.405486Z"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "import h5py"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0i4e7TnFQFPh",
    "outputId": "d8ec8642-582a-4a35-f46c-1c8844066434",
    "ExecuteTime": {
     "end_time": "2025-07-04T12:13:08.948506Z",
     "start_time": "2025-07-04T12:13:08.945816Z"
    }
   },
   "source": [
    "img_path = \"/media/ryqc/datasets/img_codes.hdf5\"\n",
    "caption_path = \"/media/ryqc/datasets/captions_tokenized.json\""
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2i1vHqOMQFPh"
   },
   "source": [
    "## Data structure (5p)\n",
    "\n",
    "To save your time, we've already vectorized all MSCOCO17 images with a pre-trained VGG19 network from [torchvision](https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py).\n",
    "\n",
    "The whole process takes anywhere between a day on CPU and 30min on 1x GeForce GTX 1060. If you want to play with that yourself, [you're welcome](https://drive.google.com/file/d/1NLtoLqZzeES_flgs9kmmeZD_tsCTenH1/view?usp=sharing).\n",
    "\n",
    "Please either download data from [here](https://drive.google.com/drive/folders/1ZDxySDJJcUbdiHqKxUl5BFERN5Lhf7Lk?usp=sharing) or generate it manually using the above script."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XtvDi1WKQFPh",
    "ExecuteTime": {
     "end_time": "2025-07-04T12:13:16.009072Z",
     "start_time": "2025-07-04T12:13:15.574188Z"
    }
   },
   "source": [
    "f = h5py.File(img_path, 'r')\n",
    "img_codes = f['data']\n"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = '/media/ryqc/datasets/img_codes.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m f = \u001B[43mh5py\u001B[49m\u001B[43m.\u001B[49m\u001B[43mFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mr\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m img_codes = f[\u001B[33m'\u001B[39m\u001B[33mdata\u001B[39m\u001B[33m'\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/myproject-env-py3118/lib/python3.11/site-packages/h5py/_hl/files.py:564\u001B[39m, in \u001B[36mFile.__init__\u001B[39m\u001B[34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001B[39m\n\u001B[32m    555\u001B[39m     fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001B[32m    556\u001B[39m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001B[32m    557\u001B[39m                      alignment_threshold=alignment_threshold,\n\u001B[32m    558\u001B[39m                      alignment_interval=alignment_interval,\n\u001B[32m    559\u001B[39m                      meta_block_size=meta_block_size,\n\u001B[32m    560\u001B[39m                      **kwds)\n\u001B[32m    561\u001B[39m     fcpl = make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001B[32m    562\u001B[39m                      fs_persist=fs_persist, fs_threshold=fs_threshold,\n\u001B[32m    563\u001B[39m                      fs_page_size=fs_page_size)\n\u001B[32m--> \u001B[39m\u001B[32m564\u001B[39m     fid = \u001B[43mmake_fid\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muserblock_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfapl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfcpl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mswmr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mswmr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    566\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(libver, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    567\u001B[39m     \u001B[38;5;28mself\u001B[39m._libver = libver\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/myproject-env-py3118/lib/python3.11/site-packages/h5py/_hl/files.py:238\u001B[39m, in \u001B[36mmake_fid\u001B[39m\u001B[34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001B[39m\n\u001B[32m    236\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m swmr \u001B[38;5;129;01mand\u001B[39;00m swmr_support:\n\u001B[32m    237\u001B[39m         flags |= h5f.ACC_SWMR_READ\n\u001B[32m--> \u001B[39m\u001B[32m238\u001B[39m     fid = \u001B[43mh5f\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfapl\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfapl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    239\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m mode == \u001B[33m'\u001B[39m\u001B[33mr+\u001B[39m\u001B[33m'\u001B[39m:\n\u001B[32m    240\u001B[39m     fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mh5py/_objects.pyx:56\u001B[39m, in \u001B[36mh5py._objects.with_phil.wrapper\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mh5py/_objects.pyx:57\u001B[39m, in \u001B[36mh5py._objects.with_phil.wrapper\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mh5py/h5f.pyx:102\u001B[39m, in \u001B[36mh5py.h5f.open\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/media/ryqc/datasets/img_codes.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0AZNIzSGQFPh",
    "ExecuteTime": {
     "end_time": "2025-06-26T20:21:18.240169Z",
     "start_time": "2025-06-26T20:21:18.215533Z"
    }
   },
   "source": [
    "#captions = json.load(open('captions_tokenized.json'))\n",
    "with open(caption_path, 'r') as f_json:\n",
    "    captions = json.load(f_json)\n"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/ryqc/datasets/captions_tokenized.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#captions = json.load(open('captions_tokenized.json'))\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcaption_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f_json:\n\u001B[1;32m      3\u001B[0m     captions \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f_json)\n",
      "File \u001B[0;32m~/PycharmProjects/Machine-Learning-Learning-Center/venv_/myenv/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    279\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    280\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    282\u001B[0m     )\n\u001B[0;32m--> 284\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/media/ryqc/datasets/captions_tokenized.json'"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j820xVHbQFPh",
    "outputId": "92b666a1-972e-447e-8af0-7d35da2e5f37",
    "ExecuteTime": {
     "end_time": "2025-06-26T20:21:18.368820Z",
     "start_time": "2025-06-26T20:21:18.351633Z"
    }
   },
   "source": [
    "print(\"Each image code is a 512x9x9-unit tensor [ shape: %s x %s ]\" % (str(len(img_codes)), str(img_codes[0].shape)))\n",
    "print(img_codes[0][:3].round(2), end='\\n\\n')\n",
    "print(\"For each image there are 5 reference captions, e.g.:\\n\")\n",
    "print('\\n'.join(captions[0]))"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_codes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEach image code is a 512x9x9-unit tensor [ shape: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m x \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m ]\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[43mimg_codes\u001B[49m)), \u001B[38;5;28mstr\u001B[39m(img_codes[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape)))\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(img_codes[\u001B[38;5;241m0\u001B[39m][:\u001B[38;5;241m3\u001B[39m]\u001B[38;5;241m.\u001B[39mround(\u001B[38;5;241m2\u001B[39m), end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFor each image there are 5 reference captions, e.g.:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'img_codes' is not defined"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cC0pHhu5QFPi"
   },
   "source": [
    "As you can see, all captions are already tokenized and lowercased. We now want to split them and add some special tokens for start/end of caption."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Sv-vhy5pQFPi",
    "ExecuteTime": {
     "end_time": "2025-06-26T20:21:18.940785Z",
     "start_time": "2025-06-26T20:21:18.920040Z"
    }
   },
   "source": [
    "#split descriptions into tokens\n",
    "for img_i in range(len(captions)):\n",
    "    for caption_i in range(len(captions[img_i])):\n",
    "        sentence = captions[img_i][caption_i]\n",
    "        captions[img_i][caption_i] = [\"#START#\"] + sentence.split(' ') + [\"#END#\"]\n",
    "\n",
    "captions[0][0] # example to see what is going on"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'captions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#split descriptions into tokens\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m img_i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[43mcaptions\u001B[49m)):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m caption_i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(captions[img_i])):\n\u001B[1;32m      4\u001B[0m         sentence \u001B[38;5;241m=\u001B[39m captions[img_i][caption_i]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'captions' is not defined"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjZUyCvjQFPi"
   },
   "source": [
    "You don't want your network to predict a million-size vector of probabilities at each step, so we're gotta make some cuts.\n",
    "\n",
    "We want you to __count the occurences of each word__ so that we can decide which words to keep in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eYZBv174QFPi",
    "ExecuteTime": {
     "end_time": "2025-06-26T20:21:19.137285Z",
     "start_time": "2025-06-26T20:21:19.110528Z"
    }
   },
   "source": [
    "# Build a Vocabulary\n",
    "from collections import Counter\n",
    "word_counts: Counter = Counter()\n",
    "\n",
    "#Compute word frequencies for each word in captions. See code above for data structure\n",
    "# <YOUR CODE HERE>\n",
    "for img_captions in captions:\n",
    "    for caption in img_captions:\n",
    "        word_counts.update(caption)\n",
    "print(word_counts.most_common(10))"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'captions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 7\u001B[0m\n\u001B[1;32m      3\u001B[0m word_counts: Counter \u001B[38;5;241m=\u001B[39m Counter()\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m#Compute word frequencies for each word in captions. See code above for data structure\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# <YOUR CODE HERE>\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m img_captions \u001B[38;5;129;01min\u001B[39;00m \u001B[43mcaptions\u001B[49m:\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m caption \u001B[38;5;129;01min\u001B[39;00m img_captions:\n\u001B[1;32m      9\u001B[0m         word_counts\u001B[38;5;241m.\u001B[39mupdate(caption)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'captions' is not defined"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "euEoMx8LQFPi",
    "ExecuteTime": {
     "end_time": "2025-06-26T20:21:19.326366Z",
     "start_time": "2025-06-26T20:21:19.304761Z"
    }
   },
   "source": [
    "vocab  = ['#UNK#', '#START#', '#END#', '#PAD#']\n",
    "vocab += [k for k, v in word_counts.items() if v >= 5 if k not in vocab]\n",
    "n_tokens = len(vocab)\n",
    "\n",
    "assert 10000 <= n_tokens <= 10500\n",
    "\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)}"
   ],
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m vocab \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [k \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m word_counts\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m v \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m vocab]\n\u001B[1;32m      3\u001B[0m n_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(vocab)\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;241m10000\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m n_tokens \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10500\u001B[39m\n\u001B[1;32m      7\u001B[0m word_to_index \u001B[38;5;241m=\u001B[39m {w: i \u001B[38;5;28;01mfor\u001B[39;00m i, w \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(vocab)}\n",
      "\u001B[0;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j7gVbDs4QFPi",
    "ExecuteTime": {
     "end_time": "2025-06-26T20:21:19.509635Z",
     "start_time": "2025-06-26T20:21:19.483016Z"
    }
   },
   "source": [
    "eos_ix = word_to_index['#END#']\n",
    "unk_ix = word_to_index['#UNK#']\n",
    "pad_ix = word_to_index['#PAD#']\n",
    "\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "\n",
    "    matrix = np.zeros((len(sequences), max_len), dtype='int32') + pad_ix\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [word_to_index.get(word, unk_ix) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "\n",
    "    return matrix\n",
    "\n",
    "# yields a 2D NumPy array where:\n",
    "# Each row is a caption (as word indices)\n",
    "# All captions are of equal length (padded with pad_ix)\n",
    "# Unknown words are replaced with unk_ix"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_to_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m eos_ix \u001B[38;5;241m=\u001B[39m \u001B[43mword_to_index\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m#END#\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      2\u001B[0m unk_ix \u001B[38;5;241m=\u001B[39m word_to_index[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m#UNK#\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      3\u001B[0m pad_ix \u001B[38;5;241m=\u001B[39m word_to_index[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m#PAD#\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'word_to_index' is not defined"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uK0jxWa4QFPi",
    "outputId": "a47bf9cc-89ac-47a5-c56b-6d96fecfb54a",
    "ExecuteTime": {
     "end_time": "2025-06-26T20:21:19.634895Z",
     "start_time": "2025-06-26T20:21:19.617198Z"
    }
   },
   "source": [
    "#try it out on several descriptions of a random image\n",
    "as_matrix(captions[1337])"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'as_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#try it out on several descriptions of a random image\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mas_matrix\u001B[49m(captions[\u001B[38;5;241m1337\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'as_matrix' is not defined"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEeypkUQQFPi"
   },
   "source": [
    "## Building our neural network (8p)\n",
    "\n",
    "As we mentioned earlier, we shall build an rnn \"language-model\" conditioned on the features from the convolutional part.\n",
    "\n",
    "We'll unbox the inception net later to save memory, for now just pretend that it's available."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hjJBenX9QFPi",
    "ExecuteTime": {
     "end_time": "2025-06-26T20:21:22.428908Z",
     "start_time": "2025-06-26T20:21:19.785938Z"
    }
   },
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mF\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmath\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/Machine-Learning-Learning-Center/venv_/myenv/lib/python3.8/site-packages/torch/__init__.py:1829\u001B[0m\n\u001B[1;32m   1824\u001B[0m         backend \u001B[38;5;241m=\u001B[39m _TorchCompileWrapper(backend, mode, options, dynamic)\n\u001B[1;32m   1826\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39moptimize(backend\u001B[38;5;241m=\u001B[39mbackend, nopython\u001B[38;5;241m=\u001B[39mfullgraph, dynamic\u001B[38;5;241m=\u001B[39mdynamic, disable\u001B[38;5;241m=\u001B[39mdisable)(model)\n\u001B[0;32m-> 1829\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m export \u001B[38;5;28;01mas\u001B[39;00m export\n\u001B[1;32m   1831\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_higher_order_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cond\n\u001B[1;32m   1833\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_register_device_module\u001B[39m(device_type, module):\n",
      "File \u001B[0;32m~/PycharmProjects/Machine-Learning-Learning-Center/venv_/myenv/lib/python3.8/site-packages/torch/export/__init__.py:62\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msymbolic_shapes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StrictMinMaxConstraint\n\u001B[1;32m     45\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConstraint\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDim\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msave\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     59\u001B[0m ]\n\u001B[0;32m---> 62\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexported_program\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ExportedProgram, ModuleCallEntry, ModuleCallSignature\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgraph_signature\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ExportBackwardSignature, ExportGraphSignature\n\u001B[1;32m     66\u001B[0m PassType \u001B[38;5;241m=\u001B[39m Callable[[torch\u001B[38;5;241m.\u001B[39mfx\u001B[38;5;241m.\u001B[39mGraphModule], Optional[PassResult]]\n",
      "File \u001B[0;32m~/PycharmProjects/Machine-Learning-Learning-Center/venv_/myenv/lib/python3.8/site-packages/torch/export/exported_program.py:30\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_pytree\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpytree\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_compatibility\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compatibility\n\u001B[0;32m---> 30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mproxy_tensor\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m maybe_disable_fake_tensor_mode\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpasses\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minfra\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpass_base\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PassResult\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpasses\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minfra\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpass_manager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PassManager\n",
      "File \u001B[0;32m~/PycharmProjects/Machine-Learning-Learning-Center/venv_/myenv/lib/python3.8/site-packages/torch/fx/experimental/proxy_tensor.py:32\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moverrides\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TorchFunctionMode\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_python_dispatch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     27\u001B[0m     TorchDispatchMode,\n\u001B[1;32m     28\u001B[0m     _pop_mode,\n\u001B[1;32m     29\u001B[0m     _push_mode,\n\u001B[1;32m     30\u001B[0m )\n\u001B[0;32m---> 32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msym_node\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SymNode\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_sym_dispatch_mode\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SymDispatchMode\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Proxy\n",
      "File \u001B[0;32m~/PycharmProjects/Machine-Learning-Learning-Center/venv_/myenv/lib/python3.8/site-packages/torch/fx/experimental/sym_node.py:33\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# NB: The sym_* functions are used via getattr() and must be imported here.\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[1;32m     22\u001B[0m     sym_float,\n\u001B[1;32m     23\u001B[0m     sym_ite,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     30\u001B[0m     SymInt,\n\u001B[1;32m     31\u001B[0m )\n\u001B[0;32m---> 33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_sym_dispatch_mode\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     34\u001B[0m     handle_sym_dispatch,\n\u001B[1;32m     35\u001B[0m     sym_function_mode,\n\u001B[1;32m     36\u001B[0m )\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msymbolic_shapes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ShapeEnv\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:991\u001B[0m, in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:975\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:671\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[0;34m(spec)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap_external>:779\u001B[0m, in \u001B[0;36mexec_module\u001B[0;34m(self, module)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap_external>:874\u001B[0m, in \u001B[0;36mget_code\u001B[0;34m(self, fullname)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap_external>:973\u001B[0m, in \u001B[0;36mget_data\u001B[0;34m(self, path)\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIw7PDMrQFPi"
   },
   "source": [
    "### Attention\n",
    "\n",
    "There are $K$ objects that you can pay attention to.\n",
    "Each object is characterized by the key $k_i$ and the value $v_i$.\n",
    "The attention layer proceeds queries.\n",
    "For the query $q$, the layer returns a weighted sum of the values of the objects, with weights proportional to the degree of key matching the query:\n",
    "$$w_i = \\frac{\\exp(score(q, k_i))}{\\sum_{j=1}^K\\exp(score(q, k_j))}$$\n",
    "$$a = \\sum_{i=1}^K w_i v_i$$\n",
    "\n",
    "Here we use $score(q, k) = \\frac{q^Tk}{\\sqrt{dim(k)}}$, where $dim(k)$ is the dimensionality of the key (which also equals the dimensionality of the query).\n",
    "For more information see the paper Vaswani et al. \"Attention Is All You Need\", 2017.\n",
    "\n",
    "_Hint:_ It is recommended to pay attention to the function torch.bmm, it may be useful below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91R_kfbhQFPi"
   },
   "source": [
    "#### Score function layer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iqqUyXoWQFPi",
    "ExecuteTime": {
     "end_time": "2025-06-26T20:21:22.432199Z",
     "start_time": "2025-06-26T20:21:22.432065Z"
    }
   },
   "source": [
    "class ScaledDotProductScore(nn.Module):\n",
    "    \"\"\"\n",
    "    Vaswani et al. \"Attention Is All You Need\", 2017.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, queries, keys):\n",
    "        \"\"\"\n",
    "        queries:  [batch_size x num_queries x dim]\n",
    "        keys:     [batch_size x num_objects x dim]\n",
    "        Returns a tensor of scores with shape [batch_size x num_queries x num_objects].\n",
    "        \"\"\"\n",
    "        dim = queries.size(-1) # size of last dim of query..why ? \n",
    "        scores = torch.bmm(queries, keys.transpose(1,2))\n",
    "        scores = scores / math.sqrt(dim)\n",
    "        # <YOUR CODE HERE>\n",
    "        return scores"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqUu_hqOQFPi"
   },
   "source": [
    "Test for ScaledDotProductScore"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WrhJaCDBQFPi",
    "outputId": "625c2e75-1ede-4fbb-eb47-224870b293a6"
   },
   "source": [
    "q = torch.tensor([[\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [1, 0, 0, 0, 0],\n",
    "]], dtype=torch.float32)\n",
    "o = torch.tensor([[\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 1, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 0, 0],\n",
    "]], dtype=torch.float32)\n",
    "print(ScaledDotProductScore()(q, o))\n",
    "print(ScaledDotProductScore()(q, o).shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAXTX6tzQFPi"
   },
   "source": [
    "#### Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IoFT3GiDQFPi"
   },
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, scorer):\n",
    "        super().__init__()\n",
    "        self.scorer = scorer\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"\n",
    "        queries:         [batch_size x num_queries x query_feature_dim]\n",
    "        keys:            [batch_size x num_objects x key_feature_dim]\n",
    "        values:          [batch_size x num_objects x obj_feature_dim]\n",
    "        Returns matrix of responses for queries with shape [batch_size x num_queries x obj_feature_dim].\n",
    "        Saves detached weights as self.attention_map.\n",
    "        \"\"\"\n",
    "        scores = self.scorer(queries, keys) # [B x Q x K]\n",
    "        # <YOUR CODE HERE>\n",
    "        weights = F.softmax(scores, dim=-1) # [B x Q x K]\n",
    "        self.attention_map = weights.detach()\n",
    "        result = torch.bmm(weights, values) # [B x Q x D_v]\n",
    "        # <YOUR CODE HERE>\n",
    "        return result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6K3DBc6WQFPi"
   },
   "source": [
    "Tests for Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bJ_plOhvQFPi"
   },
   "source": [
    "scorer = ScaledDotProductScore()\n",
    "attn = Attention(scorer)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SooJZslsQFPi"
   },
   "source": [
    "q = torch.randn(2, 3, 5)\n",
    "k = torch.randn(2, 4, 5)\n",
    "v = torch.randn(2, 4, 7)\n",
    "print(attn(q, k, v))\n",
    "assert attn(q, k, v).shape == (2, 3, 7)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DZc7AiHQQFPi"
   },
   "source": [
    "q = torch.tensor([[\n",
    "    [0.01],\n",
    "    [1],\n",
    "    [100],\n",
    "]], dtype=torch.float32)\n",
    "o = torch.tensor([[\n",
    "    [-1],\n",
    "    [0],\n",
    "    [1],\n",
    "]], dtype=torch.float32) * 1000\n",
    "a = attn(q, o, o)\n",
    "assert torch.isnan(attn.attention_map).sum() == 0\n",
    "assert torch.isnan(a).sum() == 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Kk1-mQyuQFPj",
    "outputId": "87f7b02e-918e-4fa7-d382-cb155a59519f"
   },
   "source": [
    "q = torch.tensor([[\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "]], dtype=torch.float32)\n",
    "k = torch.tensor([[\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "]], dtype=torch.float32)\n",
    "v = torch.tensor([[\n",
    "    [0],\n",
    "    [1],\n",
    "    [2],\n",
    "]], dtype=torch.float32)\n",
    "a = attn(q, k, v)\n",
    "print('Attention map:\\n', attn.attention_map)\n",
    "print('Responses:\\n', a)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhI-n778QFPj"
   },
   "source": [
    "### Language model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xSIoJ5KMQFPj"
   },
   "source": [
    "class CaptionNet(nn.Module):\n",
    "    def __init__(self, n_tokens=n_tokens, emb_size=128, lstm_units=256, cnn_channels=512):\n",
    "        \"\"\" A recurrent 'head' network for image captioning. Read scheme below. \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "\n",
    "        # a layer that converts conv features to\n",
    "        self.cnn_to_h0 = nn.Linear(cnn_channels, lstm_units)\n",
    "        self.cnn_to_c0 = nn.Linear(cnn_channels, lstm_units)\n",
    "        #The initial hidden state (h0) and The initial cell state (c0) of the LSTM.\n",
    "\n",
    "        # recurrent part, please create the layers as per scheme above.\n",
    "\n",
    "        # create embedding for input words. Use the parameters (e.g. emb_size).\n",
    "        self.emb = nn.Embedding(n_tokens, emb_size) # <YOUR CODE>\n",
    "\n",
    "        # attention: create attention over image spatial positions\n",
    "        # The query is previous lstm hidden state, the keys are transformed cnn features,\n",
    "        # the values are cnn features\n",
    "        self.attention = Attention(scorer)# <YOUR CODE>\n",
    "\n",
    "        # attention: create transform from cnn features to the keys\n",
    "        # Hint: one linear layer shoud work\n",
    "        # Hint: the dimensionality of keys should be lstm_units as lstm\n",
    "        # hidden state is the attention query\n",
    "        self.cnn_to_attn_key = nn.Linear(cnn_channels, lstm_units)# This projects the CNN features into the key space, so they match the dimensionality of the query (hidden state of LSTM).\n",
    "\n",
    "        # lstm: create a recurrent core of your network. Use LSTMCell\n",
    "        self.lstm = nn.LSTMCell(emb_size + cnn_channels, lstm_units) # <YOUR CODE>\n",
    "\n",
    "        # create logits: MLP that takes attention response, lstm hidden state\n",
    "        # and the previous word embedding as an input and computes one number per token\n",
    "        # Hint: I used an architecture with one hidden layer, but you may try deeper ones\n",
    "        self.logits_mlp = nn.Sequential(\n",
    "            nn.Linear(cnn_channels+ lstm_units + emb_size , cnn_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(cnn_channels, n_tokens)\n",
    "        )# <YOUR CODE>\n",
    "\n",
    "    def forward(self, image_features, captions_ix):\n",
    "        \"\"\"\n",
    "        Apply the network in training mode.\n",
    "        :param image_features: torch tensor containing VGG features for each position.\n",
    "                               shape: [batch, cnn_channels, width * height]\n",
    "        :param captions_ix: torch tensor containing captions as matrix. shape: [batch, word_i].\n",
    "            padded with pad_ix\n",
    "        :returns: logits for next token at each tick, shape: [batch, word_i, n_tokens]\n",
    "        \"\"\"\n",
    "        initial_cell = self.cnn_to_c0(image_features.mean(2))\n",
    "        initial_hid = self.cnn_to_h0(image_features.mean(2)) #Average the CNN features across spatial dimensions to get a global image summary, then convert to LSTM’s initial state.\n",
    "\n",
    "        image_features = image_features.transpose(1, 2)# change shape from [B, C, HxW] → [B, HxW, C] so that attention can operate over spatial positions.\n",
    "\n",
    "        captions_emb = self.emb(captions_ix)  # [B, T, emb_size]\n",
    "        keys = self.cnn_to_attn_key(image_features)  # [B, HxW, lstm_units] Transform CNN features into keys for attention (so they’re compatible with LSTM hidden state queries).\n",
    "\n",
    "        h, c = initial_hid, initial_cell\n",
    "        lstm_out = []\n",
    "        attention_weights = []\n",
    "\n",
    "        # apply recurrent layer to captions_emb.\n",
    "        # 1. initialize lstm state with initial_* from above\n",
    "        # 2. In the recurrent loop over tokens:\n",
    "        #   2.1. transform image vectors to the keys for attention\n",
    "        #   2.2. use previous lstm state as an attention query and image vectors as values\n",
    "        #   2.3. apply attention to obtain context vector\n",
    "        #   2.4. store attention map\n",
    "        #   2.5. feed lstm with current token embedding concatenated with context vector\n",
    "        #   2.6. update lstm hidden and cell vectors\n",
    "        #   2.7. store current lstm hidden state, attention response, and the previous word embedding\n",
    "        # reccurent_out should be lstm hidden state sequence\n",
    "        # of shape [batch, caption_length, lstm_units + cnn_channels + emb_size]\n",
    "        # attention_map should be attention maps sequence\n",
    "        # of shape [batch, caption_length, width * height]\n",
    "        for t in range(captions_emb.shape[1]):\n",
    "            word_emb = captions_emb[:, t, :]  # [B, emb_size]\n",
    "            queries = h.unsqueeze(1)  # [B, 1, lstm_units]Get the embedding of the current word.Turn LSTM hidden state into a query for attention.\n",
    "\n",
    "            context = self.attention(queries, keys, image_features)  # [B, 1, cnn_channels]\n",
    "            context = context.squeeze(1)  # [B, cnn_channels]\n",
    "            attention_weights.append(self.attention.attention_map.squeeze(1))  # [B, HxW]\n",
    "\n",
    "            lstm_input = torch.cat([word_emb, context], dim=-1)  # [B, emb+cnn]\n",
    "            h, c = self.lstm(lstm_input, (h, c))  # h: [B, lstm_units]\n",
    "\n",
    "            lstm_out.append(torch.cat([h, context, word_emb], dim=-1))  # [B, lstm+cnn+emb] we save everything needed to produce logits later.\n",
    "\n",
    "        recurrent_out = torch.stack(lstm_out, dim=1)  # [B, T, lstm+cnn+emb]\n",
    "        attention_map = torch.stack(attention_weights, dim=1)  # [B, T, HxW]\n",
    "\n",
    "        logits = self.logits_mlp(recurrent_out)  # [B, T, n_tokens]\n",
    "\n",
    "        return logits, attention_map"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D7zrPBBCQFPj"
   },
   "source": [
    "network = CaptionNet(n_tokens)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E6KWZXKVQFPj",
    "outputId": "493c391b-64d0-4cd1-a69b-c027c0c17bc6"
   },
   "source": [
    "dummy_img_vec = torch.randn(len(captions[0]), 512, 81)\n",
    "dummy_capt_ix = torch.tensor(as_matrix(captions[0]), dtype=torch.int64)\n",
    "\n",
    "dummy_logits, dummy_attention_map = network.forward(dummy_img_vec, dummy_capt_ix)\n",
    "dummy_logits, dummy_attention_map = network.forward(dummy_img_vec, dummy_capt_ix)\n",
    "\n",
    "print('logits shape:', dummy_logits.shape)\n",
    "print('attention map shape:', dummy_attention_map.shape)\n",
    "assert dummy_logits.shape == (dummy_capt_ix.shape[0], dummy_capt_ix.shape[1], n_tokens)\n",
    "assert dummy_attention_map.shape == (dummy_capt_ix.shape[0], dummy_capt_ix.shape[1], 81)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgjrQFuiQFPj"
   },
   "source": [
    "#### Train loss function"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d9n1u8bxQFPj"
   },
   "source": [
    "def compute_loss(network, image_features, captions_ix):\n",
    "    \"\"\"\n",
    "    :param image_features: torch tensor containing VGG features. shape: [batch, cnn_channels, width * height]\n",
    "    :param captions_ix: torch tensor containing captions as matrix. shape: [batch, word_i].\n",
    "        padded with pad_ix\n",
    "    :returns: crossentropy (neg llh) loss for next captions_ix given previous ones plus\n",
    "              attention regularizer. Scalar float tensor\n",
    "    \"\"\"\n",
    "\n",
    "    if next(network.parameters()).is_cuda:\n",
    "        image_features, captions_ix = image_features.cuda(), captions_ix.cuda()\n",
    "\n",
    "    # captions for input - all except last cuz we don't know next token for last one.\n",
    "    captions_ix_inp = captions_ix[:, :-1].contiguous()\n",
    "    captions_ix_next = captions_ix[:, 1:].contiguous()\n",
    "\n",
    "    # apply the network, get predictions, attnetion map and gates for captions_ix_next\n",
    "    logits_for_next, attention_map = network.forward(image_features, captions_ix_inp)\n",
    "\n",
    "\n",
    "    # compute the loss function between logits_for_next and captions_ix_next\n",
    "    # Use the mask, Luke: make sure that predicting next tokens after EOS do not contribute to loss\n",
    "    # you can do that either by multiplying elementwise loss by (captions_ix_next != pad_ix)\n",
    "    # or by using ignore_index in some losses.\n",
    "    B, T, V = logits_for_next.shape\n",
    "    logits_flat = logits_for_next.view(B * T, V)\n",
    "    targets_flat = captions_ix_next.view(B * T)\n",
    "\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=pad_ix)# <YOUR CODE>\n",
    "\n",
    "    # the regularizer for attention - this one requires the attention over each position to sum up to 1,\n",
    "    # i. e. to look at the whole image during sentence generation process\n",
    "    mask = (captions_ix_inp != pad_ix) #B, T]\n",
    "    masked_attention_map = attention_map * mask[:, :, None].float() #[B, T, 81]\n",
    "    regularizer = ((1 - masked_attention_map.sum(1)) ** 2).mean() #B, 81]\n",
    "\n",
    "    return loss + regularizer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1e3FSMe9QFPj"
   },
   "source": [
    "dummy_loss = compute_loss(network, dummy_img_vec, dummy_capt_ix)\n",
    "\n",
    "assert len(dummy_loss.shape) <= 1, 'loss must be scalar'\n",
    "assert dummy_loss.detach().cpu().numpy() > 0, \"did you forget the 'negative' part of negative log-likelihood\"\n",
    "\n",
    "dummy_loss.backward()\n",
    "\n",
    "assert all(param.grad is not None for param in network.parameters()), \\\n",
    "        'loss should depend differentiably on all neural network weights'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sTjuYTZQFPj"
   },
   "source": [
    "#### Optimizer\n",
    "Create ~~adam~~ your favorite optimizer for the network."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tSSadKcqQFPm"
   },
   "source": "optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teXynX69QFPm"
   },
   "source": [
    "# Training (7p)\n",
    "\n",
    "* First make train/val split without extra memory usage\n",
    "* Implement the batch generator\n",
    "* Than train the network as usual"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ly3mOtzLQFPm"
   },
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IdxDataset(Dataset):\n",
    "    def __init__(self, dataset, idx):\n",
    "        self.dataset = dataset\n",
    "        self.idx = idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.idx[idx]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PEOvFkQhQFPm"
   },
   "source": [
    "# captions = np.array(captions) # we dont need this since our captions can have a different number of tokens, so np.array() needs idk something uniform i guess\n",
    "\n",
    "np.random.seed(42)\n",
    "perm = np.random.permutation(len(img_codes))\n",
    "threshold = round(len(img_codes) * 0.1)\n",
    "train_img_idx, val_img_idx = perm[threshold:], perm[: threshold]\n",
    "\n",
    "train_img_idx.sort()\n",
    "val_img_idx.sort()\n",
    "train_img_codes = IdxDataset(img_codes, train_img_idx)\n",
    "val_img_codes = IdxDataset(img_codes, val_img_idx)\n",
    "train_captions = IdxDataset(captions, train_img_idx)\n",
    "val_captions = IdxDataset(captions, val_img_idx)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TMXenoRbQFPm"
   },
   "source": [
    "from random import choice\n",
    "\n",
    "last_batch_end = {}\n",
    "\n",
    "def generate_batch(img_codes, captions, batch_size, max_caption_len=None):\n",
    "\n",
    "    #sample sequential numbers for image/caption indicies (for trainign speed up)\n",
    "    global last_batch_end\n",
    "    random_image_ix = np.arange(batch_size, dtype='int') + last_batch_end.get(len(img_codes), 0)\n",
    "    last_batch_end[len(img_codes)] = last_batch_end.get(len(img_codes), 0) + batch_size\n",
    "    if last_batch_end[len(img_codes)] + batch_size >= len(img_codes):\n",
    "        last_batch_end[len(img_codes)] = 0\n",
    "\n",
    "    #get images\n",
    "    batch_images = np.vstack([img_codes[i][None] for i in random_image_ix])\n",
    "    batch_images = batch_images.reshape(batch_images.shape[0], batch_images.shape[1], -1)\n",
    "\n",
    "    #5-7 captions for each image\n",
    "    #captions_for_batch_images = captions[random_image_ix] # Python lists don’t support NumPy array indexing directly.\n",
    "    captions_for_batch_images = [captions[i] for i in random_image_ix]\n",
    "\n",
    "\n",
    "\n",
    "    #pick one from a set of captions for each image\n",
    "    batch_captions = list(map(choice,captions_for_batch_images))\n",
    "\n",
    "    #convert to matrix\n",
    "    batch_captions_ix = as_matrix(batch_captions,max_len=max_caption_len)\n",
    "\n",
    "    return torch.tensor(batch_images, dtype=torch.float32), torch.tensor(batch_captions_ix, dtype=torch.int64)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tgz2eVWtQFPm",
    "outputId": "d079b8ce-57a1-4779-dfb9-390fa0930ef5"
   },
   "source": [
    "generate_batch(img_codes, captions, 3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEPV_RC9QFPm"
   },
   "source": [
    "### Main loop\n",
    "\n",
    "Train on minibatches just as usual. Evaluate on val from time to time.\n",
    "\n",
    "##### TIps\n",
    "* If training loss has become close to 0 or model produces garbage,\n",
    "    double-check that you're predicting __next__ words, not current or t+2'th words.\n",
    "* If the model generates fluent captions that have nothing to do with the images\n",
    " * this may be due to recurrent net not receiving image vectors.\n",
    " * alternatively it may be caused by gradient explosion, try clipping 'em or just restarting the training\n",
    " * finally, you may just need to train the model a bit more\n",
    "\n",
    "\n",
    "* Crossentropy is a poor measure of overfitting\n",
    " * Model can overfit validation crossentropy but keep improving validation quality.\n",
    " * Use human _(manual)_ evaluation or try automated metrics: [cider](https://github.com/vrama91/cider) or [bleu](https://www.nltk.org/_modules/nltk/translate/bleu_score.html)\n",
    "\n",
    "\n",
    "* We recommend you to periodically evaluate the network using the next \"apply trained model\" block\n",
    " *  its safe to interrupt training, run a few examples and start training again\n",
    "\n",
    "* The typical loss values should be around 3~5 if you average over time, scale by length if you sum over time. The reasonable captions began appearing at loss=3.5~3.7"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "67-Il8p8QFPm"
   },
   "source": [
    "batch_size = 64  # adjust me\n",
    "n_epochs = 20    # adjust me\n",
    "n_batches_per_epoch = 8 # adjust me\n",
    "n_validation_batches = 2  # how many batches are used for validation after each epoch"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qSBkyp3oQFPm",
    "outputId": "75f7cd3a-f88f-4903-e4f6-596b404029a5",
    "ExecuteTime": {
     "end_time": "2025-06-26T20:21:22.562125Z",
     "start_time": "2025-06-26T20:21:22.533321Z"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    if torch.cuda.is_available():\n",
    "        network = network.cuda()\n",
    "\n",
    "    train_loss=0\n",
    "    network.train(True)\n",
    "    with tqdm(range(n_batches_per_epoch)) as iterator:\n",
    "        for _ in iterator:\n",
    "            loss_t = compute_loss(network, *generate_batch(train_img_codes, train_captions, batch_size))\n",
    "            optimizer.zero_grad()\n",
    "            loss_t.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += float(loss_t)\n",
    "    train_loss /= n_batches_per_epoch\n",
    "\n",
    "    val_loss=0\n",
    "    network.train(False)\n",
    "    for _ in range(n_validation_batches):\n",
    "        loss_t = compute_loss(network, *generate_batch(val_img_codes, val_captions, batch_size))\n",
    "        val_loss += float(loss_t)\n",
    "    val_loss /= n_validation_batches\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        network = network.cpu()\n",
    "\n",
    "    print('\\nEpoch: {}, train loss: {}, val loss: {}'.format(epoch, train_loss, val_loss), flush=True)\n",
    "\n",
    "print(\"Finished!\")"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_epochs):\n\u001B[0;32m----> 4\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available():\n\u001B[1;32m      5\u001B[0m         network \u001B[38;5;241m=\u001B[39m network\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m      7\u001B[0m     train_loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61L51tWUQFPm"
   },
   "source": [
    "### Apply trained model\n",
    "\n",
    "Let's unpack our pre-trained VGG network and see what our model is capable of."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cVyh8hceQFPn",
    "ExecuteTime": {
     "end_time": "2025-06-26T20:21:22.936829Z",
     "start_time": "2025-06-26T20:21:22.709272Z"
    }
   },
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.vgg import VGG, cfgs as VGG_cfgs, make_layers\n",
    "from warnings import warn\n",
    "class BeheadedVGG19(VGG):\n",
    "    \"\"\" Like torchvision.models.inception.Inception3 but the head goes separately \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_for_attn = x= self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = x = self.classifier(x)\n",
    "        return x_for_attn, logits\n",
    "\n",
    "features_net = BeheadedVGG19(make_layers(VGG_cfgs['E'], batch_norm=False), init_weights=False)\n",
    "\n",
    "from torch.utils.model_zoo import load_url\n",
    "features_net_url = 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth'\n",
    "features_net.load_state_dict(load_url(features_net_url))\n",
    "\n",
    "features_net = features_net.train(False)\n",
    "if torch.cuda.is_available():\n",
    "    features_net = features_net.cuda()\n",
    "features_net = nn.DataParallel(features_net)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nn\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mF\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvgg\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VGG, cfgs \u001B[38;5;28;01mas\u001B[39;00m VGG_cfgs, make_layers\n",
      "File \u001B[0;32m~/PycharmProjects/Machine-Learning-Learning-Center/venv_/myenv/lib/python3.8/site-packages/torch/__init__.py:533\u001B[0m\n\u001B[1;32m    519\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(textwrap\u001B[38;5;241m.\u001B[39mdedent(\u001B[38;5;124m'''\u001B[39m\n\u001B[1;32m    520\u001B[0m \u001B[38;5;124m            Failed to load PyTorch C extensions:\u001B[39m\n\u001B[1;32m    521\u001B[0m \u001B[38;5;124m                It appears that PyTorch has loaded the `torch/_C` folder\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    529\u001B[0m \u001B[38;5;124m                or by running Python from a different directory.\u001B[39m\n\u001B[1;32m    530\u001B[0m \u001B[38;5;124m            \u001B[39m\u001B[38;5;124m'''\u001B[39m)\u001B[38;5;241m.\u001B[39mstrip()) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    531\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m  \u001B[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001B[39;00m\n\u001B[0;32m--> 533\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mdir\u001B[39m(\u001B[43m_C\u001B[49m):\n\u001B[1;32m    534\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m name\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBase\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    535\u001B[0m         __all__\u001B[38;5;241m.\u001B[39mappend(name)\n",
      "\u001B[0;31mNameError\u001B[0m: name '_C' is not defined"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5jpXy3WQFPn"
   },
   "source": [
    "### Generate caption\n",
    "\n",
    "The function below creates captions by sampling from probabilities defined by the net.\n",
    "\n",
    "The implementation used here is simple but inefficient (quadratic in lstm steps). We keep it that way since it isn't a performance bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "x7zs1m9FQFPn",
    "ExecuteTime": {
     "end_time": "2025-06-26T20:21:22.964074Z",
     "start_time": "2025-06-26T20:21:22.954458Z"
    }
   },
   "source": [
    "def generate_caption(image, caption_prefix = (\"#START#\",),\n",
    "                     t=1, sample=True, max_len=100):\n",
    "\n",
    "    assert isinstance(image, np.ndarray) and np.max(image) <= 1\\\n",
    "           and np.min(image) >=0 and image.shape[-1] == 3\n",
    "\n",
    "    image = torch.tensor(image.transpose([2, 0, 1]), dtype=torch.float32)\n",
    "\n",
    "    vectors_9x9, logits = features_net(image[None])\n",
    "    caption_prefix = list(caption_prefix)\n",
    "\n",
    "    attention_maps = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "\n",
    "        prefix_ix = as_matrix([caption_prefix])\n",
    "        prefix_ix = torch.tensor(prefix_ix, dtype=torch.int64)\n",
    "        input_features = vectors_9x9.view(vectors_9x9.shape[0], vectors_9x9.shape[1], -1)\n",
    "        if next(network.parameters()).is_cuda:\n",
    "            input_features, prefix_ix = input_features.cuda(), prefix_ix.cuda()\n",
    "        else:\n",
    "            input_features, prefix_ix = input_features.cpu(), prefix_ix.cpu()\n",
    "        next_word_logits, cur_attention_map = network(input_features, prefix_ix)\n",
    "        next_word_logits = next_word_logits[0, -1]\n",
    "        cur_attention_map = cur_attention_map[0, -1]\n",
    "        next_word_probs = F.softmax(next_word_logits, -1).detach().cpu().numpy()\n",
    "        attention_maps.append(cur_attention_map.detach().cpu())\n",
    "\n",
    "        assert len(next_word_probs.shape) ==1, 'probs must be one-dimensional'\n",
    "        next_word_probs = next_word_probs ** t / np.sum(next_word_probs ** t) # apply temperature\n",
    "\n",
    "        if sample:\n",
    "            next_word = np.random.choice(vocab, p=next_word_probs)\n",
    "        else:\n",
    "            next_word = vocab[np.argmax(next_word_probs)]\n",
    "\n",
    "        caption_prefix.append(next_word)\n",
    "\n",
    "        if next_word==\"#END#\":\n",
    "            break\n",
    "\n",
    "    return caption_prefix, attention_maps"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92t8oxVYQFPn"
   },
   "source": [
    "Here is the code which downloads image, prints different generated captions and visualize attention map."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SKUT1XEWQFPn",
    "ExecuteTime": {
     "end_time": "2025-06-26T20:21:28.644050Z",
     "start_time": "2025-06-26T20:21:23.120306Z"
    }
   },
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from tempfile import mktemp\n",
    "from os import remove\n",
    "%matplotlib inline\n",
    "\n",
    "# get and preprocess image\n",
    "def obtain_image(filename=None, url=None):\n",
    "    if (filename is None and url is None) or (filename is not None and url is not None):\n",
    "        raise ValueError('You shoud specify either filename or url')\n",
    "    if url is not None:\n",
    "        tmpfilename = mktemp()\n",
    "        !wget {url} -O {tmpfilename} -q\n",
    "        img = plt.imread(tmpfilename)\n",
    "        remove(tmpfilename)\n",
    "    else:\n",
    "        img = plt.imread(filename)\n",
    "    img = resize(img, (299, 299), mode='wrap', anti_aliasing=True).astype('float32')\n",
    "    return img\n",
    "\n",
    "def show_img(img):\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "def print_possible_captions(img, num_captions=10, temperature=5.):\n",
    "    for i in range(num_captions):\n",
    "        print(' '.join(generate_caption(img, t=temperature)[0][1:-1]))\n",
    "\n",
    "def draw_attention_map(img, caption, attention_map):\n",
    "    s = 4\n",
    "    n = len(caption)\n",
    "    w = 4\n",
    "    h = n // w + 1\n",
    "    plt.figure(figsize=(w * s, h * s))\n",
    "    plt.subplot(h, w, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title('INPUT', fontsize=s * 4)\n",
    "    plt.axis('off')\n",
    "    for i, word, attention in zip(range(n), caption, attention_map):\n",
    "        plt.subplot(h, w, 2 + i)\n",
    "        attn_map = attention.view(1, 1, 9, 9)\n",
    "        attn_map = F.interpolate(attn_map, size=(12, 12), mode='nearest')\n",
    "        attn_map = F.interpolate(attn_map, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "        attn_map = attn_map[0, 0][:, :, None]\n",
    "        attn_map = torch.min(attn_map / attn_map.max(), torch.ones_like(attn_map)).numpy()\n",
    "        plt.imshow(img * attn_map)\n",
    "        plt.title(word, fontsize=s * 4)\n",
    "        plt.axis('off')\n",
    "\n",
    "def process_image(img):\n",
    "    print_possible_captions(img)\n",
    "    c, am = generate_caption(img, t=5.)\n",
    "    draw_attention_map(img, c[1:-1], am)"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EqaxEZ76QFPn",
    "outputId": "718e4f38-9b92-466f-918c-35fdd36cb3b2",
    "ExecuteTime": {
     "end_time": "2025-06-26T20:21:29.831109Z",
     "start_time": "2025-06-26T20:21:28.649233Z"
    }
   },
   "source": [
    "img = obtain_image(url=\"https://thecenterformindfuleating.org/resources/Pictures/asian%20man%20eating.jpg\")\n",
    "show_img(img)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\r\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/private/var/folders/7t/fvj0_86968v3wh1qtwjh_kk80000gn/T/tmpb26w_897'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43mobtain_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhttps://thecenterformindfuleating.org/resources/Pictures/asian\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43m20man\u001B[39;49m\u001B[38;5;132;43;01m%20e\u001B[39;49;00m\u001B[38;5;124;43mating.jpg\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m show_img(img)\n",
      "Cell \u001B[0;32mIn[32], line 14\u001B[0m, in \u001B[0;36mobtain_image\u001B[0;34m(filename, url)\u001B[0m\n\u001B[1;32m     12\u001B[0m     tmpfilename \u001B[38;5;241m=\u001B[39m mktemp()\n\u001B[1;32m     13\u001B[0m     get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwget \u001B[39m\u001B[38;5;132;01m{url}\u001B[39;00m\u001B[38;5;124m -O \u001B[39m\u001B[38;5;132;01m{tmpfilename}\u001B[39;00m\u001B[38;5;124m -q\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 14\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtmpfilename\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m     remove(tmpfilename)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/Machine-Learning-Learning-Center/venv_/myenv/lib/python3.8/site-packages/matplotlib/pyplot.py:2195\u001B[0m, in \u001B[0;36mimread\u001B[0;34m(fname, format)\u001B[0m\n\u001B[1;32m   2193\u001B[0m \u001B[38;5;129m@_copy_docstring_and_deprecators\u001B[39m(matplotlib\u001B[38;5;241m.\u001B[39mimage\u001B[38;5;241m.\u001B[39mimread)\n\u001B[1;32m   2194\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mimread\u001B[39m(fname, \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m-> 2195\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmatplotlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Machine-Learning-Learning-Center/venv_/myenv/lib/python3.8/site-packages/matplotlib/image.py:1563\u001B[0m, in \u001B[0;36mimread\u001B[0;34m(fname, format)\u001B[0m\n\u001B[1;32m   1556\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fname, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(parse\u001B[38;5;241m.\u001B[39murlparse(fname)\u001B[38;5;241m.\u001B[39mscheme) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   1557\u001B[0m     \u001B[38;5;66;03m# Pillow doesn't handle URLs directly.\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1559\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease open the URL for reading and pass the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresult to Pillow, e.g. with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1562\u001B[0m         )\n\u001B[0;32m-> 1563\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mimg_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m image:\n\u001B[1;32m   1564\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (_pil_png_to_float_array(image)\n\u001B[1;32m   1565\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(image, PIL\u001B[38;5;241m.\u001B[39mPngImagePlugin\u001B[38;5;241m.\u001B[39mPngImageFile) \u001B[38;5;28;01melse\u001B[39;00m\n\u001B[1;32m   1566\u001B[0m             pil_to_array(image))\n",
      "File \u001B[0;32m~/PycharmProjects/Machine-Learning-Learning-Center/venv_/myenv/lib/python3.8/site-packages/PIL/Image.py:3431\u001B[0m, in \u001B[0;36mopen\u001B[0;34m(fp, mode, formats)\u001B[0m\n\u001B[1;32m   3428\u001B[0m     filename \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mrealpath(os\u001B[38;5;241m.\u001B[39mfspath(fp))\n\u001B[1;32m   3430\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filename:\n\u001B[0;32m-> 3431\u001B[0m     fp \u001B[38;5;241m=\u001B[39m \u001B[43mbuiltins\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3432\u001B[0m     exclusive_fp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   3433\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/private/var/folders/7t/fvj0_86968v3wh1qtwjh_kk80000gn/T/tmpb26w_897'"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MfvKiDjkQFPn",
    "outputId": "d6af594f-c838-4411-8f5a-1867d3a172b6"
   },
   "source": [
    "process_image(img)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fICc3p6NQFPn",
    "outputId": "fdf445eb-e9c3-4e34-b6e5-251071b35915"
   },
   "source": [
    "process_image(obtain_image(url=\"https://www.dimensioneattiva.it/wp-content/uploads/2016/10/Bike5.jpg\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "77cBnVpKQFPn",
    "outputId": "72d1968c-59d5-42c7-c17d-e6ae21458a6d"
   },
   "source": [
    "process_image(obtain_image(url=\"http://ccanimalclinic.com/wp-content/uploads/2017/07/Cat-and-dog-1.jpg\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Fq_7kMS8QFPn",
    "outputId": "1a40a553-bf82-4a84-f840-17d95b3d3a35"
   },
   "source": [
    "process_image(obtain_image(url=\"https://pixel.nymag.com/imgs/daily/selectall/2018/02/12/12-tony-hawk.w710.h473.jpg\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oHB-ARUCQFPn",
    "outputId": "e7ed3513-fd70-4d73-9c0a-d2748705049d"
   },
   "source": [
    "process_image(obtain_image(filename=\"/home/oleg/usa00/IMG_20190520_125121.jpg\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dY8kT45hQFPn"
   },
   "source": [
    "# Demo (10p)\n",
    "### Find at least 10 images to test it on.\n",
    "* Seriously, that's part of an assignment. Go get at least 10 pictures to get captioned\n",
    "* Make sure it works okay on __simple__ images before going to something more comples\n",
    "* Photos, not animation/3d/drawings, unless you want to train CNN network on anime\n",
    "* Mind the aspect ratio\n",
    "* Describe what you see and draw a conclusion"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0ZB3IqDmQFPn"
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_folder = 'ComputerVisionCourse/caption_images' \n",
    "\n",
    "def get_local_image(filepath):\n",
    "    try:\n",
    "        img = Image.open(filepath).convert('RGB')\n",
    "        img = img.resize((299, 299))\n",
    "        return np.array(img) / 255.0\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process {filepath}: {e}\")\n",
    "        return None\n",
    "    \n",
    "image_files = [f for f in os.listdir(image_folder) if f.lower().endswith('.jpeg')]\n",
    "image_files.sort()  \n",
    "image_paths = [os.path.join(image_folder, fname) for fname in image_files]\n",
    "\n",
    "imgs = [get_local_image(path) for path in image_paths]\n",
    "imgs = [img for img in imgs if img is not None]\n",
    "\n",
    "for i, img in enumerate(imgs):\n",
    "    print(f\"\\n--- Image {i+1} ---\")\n",
    "    show_img(img)\n",
    "    process_image(img)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UvCNH_V3QFPn"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aSKeVEJQFPn"
   },
   "source": [
    "### Conclusion\n",
    "Here is a place for your conclusions, observations, hypotheses, and any other feedback."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IZPqiF2QFPn"
   },
   "source": [
    "### Now what?\n",
    "\n",
    "Your model produces some captions but you still strive to improve it? You're damn right to do so. Here are some ideas that go beyond simply \"stacking more layers\". The options are listed easiest to hardest.\n",
    "\n",
    "##### Attention\n",
    "You can build better and more interpretable captioning model with attention over the generated part of the sentense.\n",
    "\n",
    "##### Subword level captioning\n",
    "In the base version, we replace all rare words with UNKs which throws away a lot of information and reduces quality. A better way to deal with vocabulary size problem would be to use Byte-Pair Encoding\n",
    "\n",
    "* BPE implementation you can use: [github_repo](https://github.com/rsennrich/subword-nmt).\n",
    "* Theory: https://arxiv.org/abs/1508.07909\n",
    "* It was originally built for machine translation, but it should work with captioning just as well.\n",
    "\n",
    "#### Reinforcement learning\n",
    "* After your model has been pre-trained in a teacher forced way, you can tune for captioning-speific models like CIDEr.\n",
    "* Tutorial on RL for sequence models: [practical_rl week8](https://github.com/yandexdataschool/Practical_RL/tree/master/week8_scst)\n",
    "* Theory: https://arxiv.org/abs/1612.00563"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
