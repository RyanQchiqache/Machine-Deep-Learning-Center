{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# I- Intro to pytorch and Convolution\n",
   "id": "d089cdee645f5889"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:01:50.323602Z",
     "start_time": "2025-07-18T12:01:46.124584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import PIL.Image as I\n",
    "import torch\n",
    "from scipy.fftpack import fft2, ifft2,ifftshift, fftshift  \n",
    "import cv2"
   ],
   "id": "8d72b95b6dc365f7",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 1 (Pseudo-Code)\n",
    "Write a pseudo-code snippet that performs the following steps:\n",
    "1. Assume you are given a NumPy array named color image with a shape of (H, W, C) where H=512, W=512, and C=3.\n",
    "2. Assume you are also given a NumPy array named gray_patch with a shape of (P, P) where P=64. \n",
    "3. Insert the gray_patch into the top-left corner of the color image. Since the patch is grayscale, its values should be replicated across all three color channels of the target region in color_image."
   ],
   "id": "68b9a003ee822d36"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-18T19:49:30.025818Z",
     "start_time": "2025-07-18T19:49:29.900770Z"
    }
   },
   "source": [
    "color_image = I.open(\"/Users/ryanqchiqache/PycharmProjects/Machine-Learning-Learning-Center/ComputerVisionCourse/Exercise02/saturn.png\").convert(\"RGB\")\n",
    "gray_patch_np = np.resize(color_image, (128,128))\n",
    "gray_patch = np.transpose(gray_patch_np, (2,2))\n",
    "replicated_patch = np.stack([gray_patch] * 3, axis=-1)\n",
    "color_image[0:64, 0:64, :] = replicated_patch"
   ],
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 2 is out of bounds for array of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAxisError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[24]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m color_image = I.open(\u001B[33m\"\u001B[39m\u001B[33m/Users/ryanqchiqache/PycharmProjects/Machine-Learning-Learning-Center/ComputerVisionCourse/Exercise02/saturn.png\u001B[39m\u001B[33m\"\u001B[39m).convert(\u001B[33m\"\u001B[39m\u001B[33mRGB\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      2\u001B[39m gray_patch_np = np.resize(color_image, (\u001B[32m128\u001B[39m,\u001B[32m128\u001B[39m))\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m gray_patch = \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgray_patch_np\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m replicated_patch = np.stack([gray_patch] * \u001B[32m3\u001B[39m, axis=-\u001B[32m1\u001B[39m)\n\u001B[32m      5\u001B[39m color_image[\u001B[32m0\u001B[39m:\u001B[32m64\u001B[39m, \u001B[32m0\u001B[39m:\u001B[32m64\u001B[39m, :] = replicated_patch\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/myproject-env-py3118/lib/python3.11/site-packages/numpy/core/fromnumeric.py:655\u001B[39m, in \u001B[36mtranspose\u001B[39m\u001B[34m(a, axes)\u001B[39m\n\u001B[32m    588\u001B[39m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_transpose_dispatcher)\n\u001B[32m    589\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtranspose\u001B[39m(a, axes=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    590\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    591\u001B[39m \u001B[33;03m    Returns an array with axes transposed.\u001B[39;00m\n\u001B[32m    592\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    653\u001B[39m \n\u001B[32m    654\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m655\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mtranspose\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/myproject-env-py3118/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59\u001B[39m, in \u001B[36m_wrapfunc\u001B[39m\u001B[34m(obj, method, *args, **kwds)\u001B[39m\n\u001B[32m     56\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, *args, **kwds)\n\u001B[32m     58\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m59\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbound\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     60\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[32m     61\u001B[39m     \u001B[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001B[39;00m\n\u001B[32m     62\u001B[39m     \u001B[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     66\u001B[39m     \u001B[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001B[39;00m\n\u001B[32m     67\u001B[39m     \u001B[38;5;66;03m# exception has a traceback chain.\u001B[39;00m\n\u001B[32m     68\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, *args, **kwds)\n",
      "\u001B[31mAxisError\u001B[39m: axis 2 is out of bounds for array of dimension 2"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 2 (Free-Form)\n",
    "- In image processing pipelines, it is common to convert image data between different data\n",
    "types and ranges. \n",
    "- Explain why you would normalize an 8-bit image array (with pixel values\n",
    "in the range [0, 255]) to a floating-point array (with values in the range [0, 1]) before applying\n",
    "a filter like a Gaussian blur."
   ],
   "id": "248b1ddbbdc8fb11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": " # NOTE We normalize 8-bit images to [0, 1] floats before applying Gaussian blur to ensure numerical precision, prevent overflow, and match the floating-point expectations of the filter.",
   "id": "6a18d3aec310e7ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 3 (Pseudo-Code)\n",
    " - Most deep learning frameworks like PyTorch expect image data in the format [Batch,\n",
    "Channels, Height, Width] (B, C, H, W)\n",
    " - while libraries like Pillow and Matplotlib often work with [Height, Width, Channels] (H, W, C). \n",
    "  - Write a pseudo-code function convert_hwc_to_bchw(image_array) that takes a single image as a NumPy array in HWC format and converts it into a PyTorch tensor suitable for a model, with a batch size of 1."
   ],
   "id": "33aec0d8000f55de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T19:57:25.187910Z",
     "start_time": "2025-07-18T19:57:25.183324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_hwc_bchw(image_array):\n",
    "    tensor = torch.from_numpy(image_array).permute(2,0,1)\n",
    "    bchw = tensor.unsqueeze(0)\n",
    "    return bchw.to(dtype=torch.float32)\n",
    "\n",
    "np_array = np.random.randint(0, 255, (128, 128, 3), dtype=np.uint8)\n",
    "\n",
    "print(convert_hwc_bchw(np_array).shape)"
   ],
   "id": "f1eff608b2763e97",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 4 (Free-Form)\n",
    "- Explain the two primary differences between a NumPy ndarray and a PyTorch Tensor that make Tensors more suitable for deep learning."
   ],
   "id": "cce54d09f06c9fce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# NOTE PyTorch tensors are better suited for deep learning because they support GPU acceleration and automatic differentiation for backpropagation.",
   "id": "76249441c17a22df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 5 (Pseudo-Code)\n",
    "- Implement a function convolve_2d(image, kernel) from scratch using basic array operations (e.g., loops and element-wise multiplication). \n",
    "- The function should perform a 2D cross-correlation (as is standard in CNNs).\n",
    "- Inputs: A 2D single-channel image (H x W NumPy array) and a 2D kernel (K x K NumPy array).\n",
    "- Output: A 2D filtered image.\n",
    "- Assumptions: Use a stride of 1 and no padding. The output image size will be smaller than the input."
   ],
   "id": "4a32490b1de6d798"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T20:04:06.632666Z",
     "start_time": "2025-07-18T20:04:06.621988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def conv_2d(img: I, kernel: np.ndarray, stride=1, bias=-1):\n",
    "    H, W = img.shape\n",
    "    kH, kW = kernel.shape\n",
    "    \n",
    "    out_h = H - kH // stride +1\n",
    "    out_w = W - kW // stride +1\n",
    "    \n",
    "    output = np.zeros((out_h, out_w))\n",
    "    \n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            region = img[i*stride:i*stride+kH, j*stride: j*stride+kW]\n",
    "            output[i, j] = np.sum(region * kernel) +bias\n",
    "            \n",
    "    return output\n",
    "           \n",
    "# Example\n",
    "\n",
    "img = np.array([[1, 2, 3],\n",
    "                [4, 5, 6],\n",
    "                [7, 8, 9]])\n",
    "\n",
    "kernel = np.array([[1, 0],\n",
    "                   [0, -1]])\n",
    "\n",
    "out = conv_2d(img, kernel, stride=1, bias=0)\n",
    "print(out)\n"
   ],
   "id": "9e2d40bbf6362c13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4. -4.]\n",
      " [-4. -4.]]\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 6 (Free-Form)\n",
    "\n",
    "The following formula defines a 2D Gaussian filter kernel:\n",
    "\n",
    "*f(x, y) = (1 / (2πσ²)) * exp(-(x² + y²) / (2σ²))*\n",
    "\n",
    "1. **What is the primary effect of applying a Gaussian filter to an image?**  \n",
    "   →  \"It smooths the image by averaging pixel intensities with a weighted kernel, reducing noise and fine detail.\"\n",
    "\n",
    "2. **How does changing the value of σ (sigma) affect this outcome?**  \n",
    "   → \" A larger σ results in more blurring, as the kernel becomes wider and includes more neighboring pixels in the averaging process. A smaller σ keeps more detail.\""
   ],
   "id": "6135091cb9b89c25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 7 (Free-Form)\n",
    "- A Laplace filter is often used for edge detection. A common 3x3 kernel for the Laplacian operator is:\n",
    "- Generated code:\n",
    "[[ 0, 1, 0],\n",
    "[ 1,-4, 1],\n",
    "[ 0, 1, 0]]\n",
    "- Explain briefly, in terms of what the filter calculates, why this kernel highlights edges and regions of rapid intensity change in an image. \n",
    "- (Hint: Think about what the filter approximates mathematically)."
   ],
   "id": "1f2db4128016fb69"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T20:04:47.402184Z",
     "start_time": "2025-07-18T20:04:47.399401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NOTE This kernel highlights edges because it approximates the **second spatial derivative** (the Laplacian) of the image. \n",
    "# NOTE It emphasizes regions where the intensity changes rapidly — i.e., edges — by subtracting the central pixel's value relative to its neighbors."
   ],
   "id": "ea3a777f31df7228",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f2a58e423613046"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 8 (Free-Form)\n",
    "\n",
    "**Two major limitations of hand-crafted filters:**\n",
    "\n",
    "1. They are **fixed and manually designed**, so they can’t adapt to complex patterns or data variability.\n",
    "2. They lack **learnability**, meaning they can’t improve over time or extract higher-level features.\n",
    "\n",
    "**CNNs overcome these by:**\n",
    "- Learning optimal filters during training through backpropagation.\n",
    "- Stacking multiple layers to extract hierarchical features (edges → textures → objects).\n"
   ],
   "id": "f2038a60fc8ce114"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " ### Question 9 (Free-Form)\n",
    "\n",
    "- conv_layer = torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2)\n",
    "1.  Explain the purpose of the in_channel and out_channel: \n",
    "     - in_channels=3: Specifies the number of channels in the input image (e.g., RGB).\n",
    "     - out_channels=32: Specifies the number of filters (feature maps) the layer will learn, resulting in 32 output channels.\n",
    "\n",
    "2. What is the shape of the learnable weight tensor in conv_layer :\n",
    "     - conv_layer.weight.shape → [32, 3, 5, 5] where 32 is the output channels (filters), 3 input channels per filter, and 5x5 kernel size"
   ],
   "id": "a94af8b4fdbf9e8f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 10 (Free-Form)\n",
    "- An input tensor with Input shape:[16, 3, 128, 128]  # (Batch, Channels, Height, Width) is passed to the con_layer of Question 9. Calculate the shape of the output tensor.\n",
    "-  out_dim = ((input_dim = 128 + 2 * padding = 2 - kernel_size = 5) / stride = 1 + ) 1 = ((128 + 4 - 5) / 1 ) + 1 =  128\n",
    "-  out_shape = [16, 32, 128, 128]"
   ],
   "id": "1570f9403214e370"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 11 (Free-Form)\n",
    "- implementation in NumPy can be identical to the output of PyTorch's nn.Conv2d layer (when using the same kernel).\n",
    " - What does this imply about the fundamental operation that nn.Conv2d performs? \n",
    "  - Why is using the PyTorch layer vastly more powerful and efficient in a deep learning context?\n",
    "#### Answer :\n",
    "- If a custom NumPy convolution matches the output of nn.Conv2d (with the same kernel), it means that nn.Conv2d performs the same fundamental operation — a cross-correlation over the input tensor.\n",
    "- However, PyTorch's Conv2d is far more powerful because:\n",
    "    - It supports GPU acceleration for large-scale, efficient training.\n",
    "    - It integrates with PyTorch's autograd system, enabling automatic differentiation and backpropagation."
   ],
   "id": "39da9615d0ef25c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# II-  Fourier Transformation",
   "id": "fe0c7c99f3932e06"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 1\n",
    "- Q: The Discrete Fourier Transform (DFT) of an image produces a complex-valued result. What are the two components called, and what does each represent in terms of the image's frequency content?\n",
    "\n",
    "- A: The two components are the magnitude spectrum, representing the strength of frequencies, and the phase spectrum, representing the spatial arrangement of those frequencies in the image."
   ],
   "id": "929ff98fc3049615"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Quesiton 2\n",
    "- Q: What is the purpose of fftshift when visualizing the 2D Fourier transform of an image? Where is the zero-frequency (DC) component located before and after applying fftshift?\n",
    "\n",
    "- A: fftshift moves the DC component from the top-left corner (default position) to the center of the frequency spectrum, making visualization of frequency content more intuitive."
   ],
   "id": "d62387cd8c1e0761"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 3\n",
    " - Q: What would the centered Fourier magnitude spectrum look like for Image A (horizontal stripes) and Image B (vertical stripes)\n",
    " - A: Image A will show strong vertical frequency components (bright vertical lines), while Image B will show strong horizontal frequency components after rotation."
   ],
   "id": "2c8d1c57b211520a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 4\n",
    "- Q: How are the magnitude and phase spectra affected by the following transformations?\n",
    "\n",
    "- Answers : \n",
    "- Rotating the image by 30 degrees: rotates both magnitude and phase spectra.\n",
    "- Flipping horizontally: mirrors the phase spectrum; magnitude remains symmetric.\n",
    "- Increasing contrast: scales the magnitude spectrum; phase remains unchanged."
   ],
   "id": "e5df0e00eab9f23f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 5\n",
    "- Q: Write a pseudo-code function fourier_denoise(image, threshold) that denoises an image by keeping only the strongest frequency components."
   ],
   "id": "9699a162b2330bd9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T20:25:18.566580Z",
     "start_time": "2025-07-18T20:25:18.556409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fourier_denoise(image, threshold):\n",
    "    F = fft2(image)\n",
    "    F_shifted = fftshift(F)\n",
    "    \n",
    "    magnitude = np.abs(F_shifted) > threshold\n",
    "    \n",
    "    F_filtered = F_shifted * magnitude\n",
    "    F_ishifted = ifftshift(F_filtered)\n",
    "    denoised = ifft2(F_ishifted)\n",
    "    \n",
    "    return np.real(denoised)\n",
    "    "
   ],
   "id": "c1bc8f51fba7bf53",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 6\n",
    "- Q: What is the difference between a low-pass filter and a high-pass filter in the Fourier domain? and what is it used for ?\n",
    "\n",
    "- A: A low-pass filter preserves low frequencies (center of the spectrum) and is used for blurring or denoising. A high-pass filter preserves high frequencies (edges of the spectrum) and is used for edge detection."
   ],
   "id": "2c65f8c4bd575058"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 7\n",
    "- Q: What is aliasing in image down-sampling, and what does the Nyquist-Shannon theorem say about avoiding it?\n",
    "- A: Aliasing occurs when high frequencies are misrepresented as low ones. To avoid it, the sampling rate must be at least twice the highest frequency present in the image."
   ],
   "id": "3e84cc943a94389e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 8\n",
    "- Q: What essential pre-processing step is required before down-sampling an image?\n",
    "- A: Low-pass filtering is needed to remove high-frequency content that would cause aliasing when the image is resized."
   ],
   "id": "80067c144bbf89c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 9\n",
    "- Q: Describe the three-step procedure to resize an image while minimizing aliasing.\n",
    "- A :"
   ],
   "id": "961bc1e3dfc0a4b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T20:30:02.963017Z",
     "start_time": "2025-07-18T20:30:02.955039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def low_pass_filter(image):\n",
    "    # Apply a Gaussian blur or other smoothing filter\n",
    "    kernel = cv2.getGaussianKernel(ksize=5, sigma=1)\n",
    "    gaussian = kernel @ kernel.T\n",
    "    return cv2.filter2D(image, -1, gaussian)\n",
    "\n",
    "def resize_nearest(image, scale):\n",
    "    # Resize using nearest-neighbor interpolation\n",
    "    new_size = (int(image.shape[1] * scale), int(image.shape[0] * scale))\n",
    "    return cv2.resize(image, new_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "def resize_with_antialiasing(image, scale):\n",
    "    # Step 1: Blur the image to remove high frequencies\n",
    "    blurred = low_pass_filter(image)\n",
    "    \n",
    "    # Step 2: Resize using nearest neighbor (safe from aliasing)\n",
    "    resized = resize_nearest(blurred, scale)\n",
    "    return resized"
   ],
   "id": "feecad404d4240c0",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 10 (Free-Form)\n",
    "\n",
    "- Q: Why does Gaussian blur before resizing improve quality, and what is this technique called?\n",
    "\n",
    "- A: Gaussian blur removes high frequencies that cause jagged edges when down-sampling. This pre-filtering step is called anti-aliasing and leads to smoother, higher-quality results."
   ],
   "id": "ff9bc6780dbbb85f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# III- Detection and Segmentation",
   "id": "9fdb3aa769bd8264"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 1\n",
    "\n",
    "- Q: What is the primary goal of the Grad-CAM technique? What two key pieces of information does it extract from a CNN, and from which parts of the computation are they obtained?\n",
    "\n",
    "- A: Grad-CAM highlights regions of the input image that are important for a specific prediction. It extracts: (1) feature maps from the forward pass, and (2) gradients of the target class score w.r.t. those feature maps from the backward pass."
   ],
   "id": "8a0014f6efc266bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 2\n",
    "\n",
    "- Q: Why is a late convolutional layer (e.g., layer4 in ResNet) typically used in Grad-CAM instead of an early layer?\n",
    "\n",
    "- A: Late layers capture more semantic information (what the object is) but have lower spatial resolution. Early layers have higher spatial resolution but low-level features (edges, textures). Grad-CAM prefers semantic relevance over pixel precision.\n",
    "\n"
   ],
   "id": "d70eb97e0a950503"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 3\n",
    "- Q: Write a pseudo-code function calculate_grad_cam(activations, gradients) that computes Grad-CAM."
   ],
   "id": "6ed6afc31375700c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:11:05.372019Z",
     "start_time": "2025-07-18T12:11:05.362669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_grad_cam(activations, gradients):\n",
    "    # Step 1: Compute global average pooling of gradients\n",
    "    alpha = gradients.mean(axis=(1, 2))  # shape: (C,)\n",
    "\n",
    "    # Step 2: Weighted sum of the activations\n",
    "    weighted_sum = (alpha[:, None, None] * activations).sum(axis=0)  # shape: (H, W)\n",
    "\n",
    "    # Step 3: Apply ReLU\n",
    "    heatmap = np.maximum(weighted_sum, 0)\n",
    "    return heatmap"
   ],
   "id": "9b923d74190856d4",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 4\n",
    "\n",
    "- Q: What is a \"hook\" in PyTorch, and why is it used in Grad-CAM?\n",
    "\n",
    "- A: A hook is a function that allows you to extract intermediate data from a model during the forward or backward pass. In Grad-CAM:\n",
    "- A forward hook captures feature maps.\n",
    "- A backward hook captures gradients w.r.t. those maps.\n",
    "\n"
   ],
   "id": "d1c4381515427ec3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 5\n",
    "- Q: What role do the final FC layer weights play in CAM, and what kind of architecture does this require?\n",
    "\n",
    "- A: CAM uses the FC weights to compute a weighted sum over the final convolutional feature maps. This requires a network with a Global Average Pooling (GAP) layer before the FC layer, as in the original CAM paper."
   ],
   "id": "e3f41a6e0d58ae78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 6\n",
    "\n",
    "- Q: Compare Grad-CAM and CAM in terms of how they compute feature importance. What makes Grad-CAM more general?\n",
    "\n",
    "- A: CAM uses FC weights directly (requires GAP + FC structure). Grad-CAM uses gradients to compute importance weights, making it applicable to any CNN architecture, not just those with GAP layers."
   ],
   "id": "48b8785a210b358c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 7\n",
    "- Q: How do you process a 7x7 class activation map to a binary 224x224 mask?\n",
    "- A: Upscale to input resolution using for example cv2.resize(cam, wanted_shape, linear) then normalize to [0,1] then binarize.astype(np.unit8)"
   ],
   "id": "f279a0cd9db23d9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 8\n",
    "- Q: Why is it important to visualize color-coded, blended segmentation masks?\n",
    "\n",
    "- A: It helps users intuitively understand where and why the model is predicting each class."
   ],
   "id": "7e7526b5ecf0ce5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 9\n",
    "- Q: How does pseudo-segmentation differ from fully-trained segmentation networks like FCNs?\n",
    "- first uses no pixel-level labels during training -> can give low-res masks\n",
    "- second uses ground truth segmentation masks -> gives high resolution segmentations\n"
   ],
   "id": "571236f75d5b4f4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# IV- Convolutional Neural Networks",
   "id": "b7bbc97d14cf3c8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 1 \n",
    "- Do exercise for Convolution calculation in sheet 4"
   ],
   "id": "4d3317a2c3d45e19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 2\n",
    "- Q: Why use stride 2 in CNNs, and what is another layer that reduces spatial dimensions?\n",
    "\n",
    "- A: Stride 2 reduces spatial dimensions to lower computational cost and to extract coarse-level features. Another example is max-pooling, which downsamples by taking the max value in local regions."
   ],
   "id": "ac3566a6237c6cd1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 3\n",
    "- Q: What features might the following 2x2 kernels detect?\n",
    "\n",
    "w1 = [[1, 0], [0, 1]]: Diagonal pattern detection (main diagonal)\n",
    "\n",
    "w2 = [[1, 0], [-1, 0]]: Vertical edge detector (difference across rows)\n",
    "\n",
    "w3 = [[1, 1], [0, 0]]: Horizontal edge detector (emphasis on top rows)"
   ],
   "id": "18d8eadfa88ec65b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 4\n",
    "- Q : Calculate the receptive field of a neuron in the final feature map of a CNN with the following architecture:\n",
    "- Layer 1: Convolution with kernel size 5x5, stride 1.\n",
    "- Layer 2: Max-Pooling with kernel size 2x2, stride 2.\n",
    "- Layer 3: Convolution with kernel size 3x3, stride 1.\n",
    "\n",
    "- A :\n",
    "- "
   ],
   "id": "fbe6ca52a0adb957"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 5\n",
    "- Q: Training accuracy is 100%, test accuracy is 75%. What is this, and how to fix it?\n",
    "\n",
    "- A: This is overfitting — the model memorizes training data. Fix it with:\n",
    "\n",
    "- Regularization (e.g., dropout, weight decay)\n",
    "\n",
    "- Data augmentation to increase variability"
   ],
   "id": "a853101ba9306f77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 6\n",
    "- Q: What’s the purpose of model.train() vs model.eval() in PyTorch?\n",
    "\n",
    "- A: These modes switch behavior:\n",
    "\n",
    "- train(): Enables dropout, batchnorm updates\n",
    "\n",
    "- eval(): Freezes dropout and batchnorm (inference mode)"
   ],
   "id": "4207faaad7e1c1d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 7, 8, 9, 10, 11\n",
    "- Questions :\n",
    "- Q7: Why use data augmentation (e.g., flipping, cropping) in CNN training?\n",
    "- Q8: What does a saliency map represent?\n",
    "- Q9: How do early vs. deep ResNet layer features differ?\n",
    "- Q10: How to reduce a [64, 56, 56] activation map for visualization?\n",
    "- Q11: How would saliency maps for 'cat' and 'dog' differ in the same image?\n",
    "\n",
    "- Answers :\n",
    "- A7: It increases data diversity, improving model generalization and robustness to variations in input images.\n",
    "- A8: It shows which input pixels most influence the output class. High values indicate strong impact on the prediction.\n",
    "- A9: Early layers detect edges, textures; deeper layers detect semantic objects or class-specific features.\n",
    "- A10: Two options: Mean over channels → single-channel heatmap or Select max or specific channel for targeted visualization\n",
    "- A11: Each highlights regions most relevant to its class — the cat map will focus on cat features, the dog map on dog features. This shows the network learns class-specific spatial cues."
   ],
   "id": "fe95ef7e09c5ae90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# V- Sequence Modeling in Vision\n",
    "- PRETTY STRAIGHT FORWARD"
   ],
   "id": "c0dca1ebf003856b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# VI Image Captioning and VIT\n",
   "id": "826dd73f6d85cd0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 1:\n",
    "- Q: A standard image captioning model is composed of an encoder and a decoder.\n",
    "1.  What is the role of the encoder (e.g., a pretrained VGG19)? What kind of representation does it produce from the input image?\n",
    "2. What is the role of the decoder (e.g., an LSTM)? What sequence does it generate?\n",
    "\n",
    "- A: Encoder (e.g., VGG19): The encoder's role is to extract meaningful visual features from the input image. A pretrained CNN like VGG19 processes the image through multiple convolutional layers and outputs a high-level, compact representation (feature map or vector) that captures semantic information.\n",
    "\n",
    "- Decoder (e.g., LSTM): The decoder generates a sequence of words (the caption) from the encoded image features. At each time step, it uses its internal state and possibly context from attention to predict the next word.\n"
   ],
   "id": "7e53d885b2c1e078"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 2:\n",
    "- Q: In an attention-based image captioning model:\n",
    "\n",
    "- How is the decoder's state at a given timestep used to create an attention distribution over the encoder's spatial feature maps?\n",
    "- What is the benefit of this attention mechanism compared to a non-attentional model?\n",
    "- A: \n",
    "- The decoder's hidden state at each timestep is used to compute attention weights over the encoder’s spatial feature map (e.g., via dot-product or additive attention). This results in a context vector which emphasizes relevant spatial locations of the image for generating the current word.\n",
    "- Attention allows the model to dynamically focus on different parts of the image when generating each word. This leads to more accurate and descriptive captions, especially for complex images, unlike a single global vector which may lose spatial detail."
   ],
   "id": "5ef64b13d1f6b443"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Quesiton 3:\n",
    "- Q: Why is using a pretrained CNN (e.g., VGG19 trained on ImageNet) effective for image captioning?\n",
    "- A:\n",
    "- Pretrained CNNs have learned general-purpose visual features from large datasets like ImageNet, such as edges, textures, shapes, and object parts. This transferred knowledge helps the captioning model recognize and describe objects in new images without training from scratch."
   ],
   "id": "d82d011cfddbf5ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 4:\n",
    "- Q: What makes an LSTM well-suited for generating word sequences? Why not use a feed-forward network?\n",
    "- A:\n",
    "- LSTMs can model long-range dependencies and remember past context through their gated memory cells, which is essential for generating coherent and grammatically correct sentences. Feed-forward networks lack this sequential memory capability.\n"
   ],
   "id": "30b712b49258ac38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 5:\n",
    "- Q: Describe the two main steps of the patch embedding process in a Vision Transformer (ViT):\n",
    "\n",
    "- A:\n",
    "- The image is divided into fixed-size patches (e.g., 16x16).\n",
    "\n",
    "- Each patch is flattened and linearly projected into a fixed-dimensional embedding vector using a learnable projection matrix.\n"
   ],
   "id": "185ff12361c669c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 6:\n",
    "-Q: \n",
    "- Why is positional embedding essential in ViTs?\n",
    "\n",
    "- Why isn't this needed in CNNs?\n",
    "- A:\n",
    "- Transformers lack inherent spatial order, so positional embeddings provide information about the position of each patch, allowing the model to reason about the image structure.\n",
    "- CNNs have spatial locality built into their architecture via the convolution operation and kernel sliding mechanism, so position information is implicitly encoded.\n"
   ],
   "id": "c96f9465f123b1d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 7:\n",
    "\n",
    "-Q: \n",
    "- What is the purpose of the [CLS] token in a ViT for classification?\n",
    "\n",
    "- How is it used for prediction?\n",
    "\n",
    "-A:\n",
    "- The [CLS] token is a learnable vector prepended to the input sequence. After the final transformer layer, it captures a summary of the entire image.\n",
    "- Its final representation is passed through a classification head (e.g., linear layer + softmax) to produce the class label."
   ],
   "id": "c86690dfe4a200ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 8:\n",
    "-Q: Name the two main sub-layers in a Transformer Encoder block and describe them:\n",
    "\n",
    "- A:\n",
    "- Multi-Head Self-Attention: Allows each patch to attend to all other patches to capture global context.\n",
    "\n",
    "- Feed-Forward Network (MLP): Applies non-linear transformations independently to each token to increase representation capacity.\n"
   ],
   "id": "7c4306a78e185baf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 9:\n",
    "- Q: \n",
    "- What inductive bias does a CNN have?\n",
    "\n",
    "- How does the lack of this bias in ViT affect its performance?\n",
    "- A:\n",
    "- CNNs assume spatial locality and translation invariance through local receptive fields and weight sharing.\n",
    "- ViTs require more data to learn spatial structure from scratch. They often underperform on small datasets but can surpass CNNs when trained on very large datasets."
   ],
   "id": "c7097aec5dd39038"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 10:\n",
    "- Q:\n",
    "- How does a CNN naturally produce a spatial feature map?\n",
    "\n",
    "- How does a ViT produce spatial features for detection?\n",
    "- A:\n",
    "- Through convolution and pooling, CNNs reduce the input image into a lower-resolution spatial map that preserves object locations.\n",
    "- The sequence of patch embeddings must be reshaped back into a grid or combined with positional information to build a spatial feature map for detection heads."
   ],
   "id": "9a3b7dec20918df4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# VII Matching:\n",
   "id": "f96c12942a296e4a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 1: SIFT (feature extraction and feature description)\n",
    "- What is a keypoint?\n",
    "- A location in the image that has distinctive texture or structure, like corners or blobs.\n",
    "\n",
    "- What is a descriptor?\n",
    "- A vector that encodes the local appearance around a keypoint. It must be robust to scale, rotation, and illumination changes.\n"
   ],
   "id": "67ddcfedfbc0915f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 2:\n",
    "- How many point pairs are needed to estimate an affine transformation?\n",
    "- At least 3 point pairs.\n",
    "\n",
    "- What happens if you use more?\n",
    "- Least squares is applied to minimize the error across all pairs, improving robustness to noise."
   ],
   "id": "d7a215b8701d5488"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 3:\n",
    "- Why does least-squares fail with outlier matches?\n",
    "- Outliers introduce large errors that skew the solution, leading to an inaccurate transformation.\n",
    "\n"
   ],
   "id": "689b401eb90868ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 4:\n",
    "- Give RANSAC algorithm in pseudocode :\n",
    "- ```\n",
    "  for N iterations:\n",
    "    sample 3 point pairs\n",
    "    fit affine model\n",
    "    count inliers (matches consistent with the model)\n",
    "    if current model has more inliers, save it\n",
    "    final_model = fit affine model on best inlier set```"
   ],
   "id": "2e4efc5964e95e5b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 5:\n",
    "- Why is RANSAC robust?\n",
    "- It focuses only on inliers and discards outliers. It avoids fitting to bad matches, unlike least-squares.\n",
    "\n"
   ],
   "id": "5af49cc2fccb07d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 6:\n",
    "- What is the inlier threshold?\n",
    "- A distance threshold to decide whether a match fits the model.\n",
    "\n",
    "- After the main RANSAC loop terminates and has identified the best set of inliers,\n",
    "what is the final step that is typically performed to get the most accurate model? Why\n",
    "is this step important?\n",
    "- Re-fit the model using all inliers for higher accuracy."
   ],
   "id": "434fe6b0a3dc3b3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 7:\n",
    "- Q :\n",
    " - Define dense optical flow. How does it differ from the sparse keypoint correspondences found in Task 1?\n",
    " - What kind of information does a dense flow field provide?\n",
    "\n",
    "- A:\n",
    "- Dense flow computes motion vectors for every pixel, while sparse methods compute motion only at selected keypoints.\n",
    "\n",
    "\n"
   ],
   "id": "b24942340ea726ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 8:\n",
    "- Explain the concept of backward image warping using an optical flow field.\n",
    "- Iterate over the target image grid, map back to the source using flow, and sample pixel values from the source."
   ],
   "id": "af52abfda110838d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 9:\n",
    "- When warping one video frame to another using optical flow, artifacts often appear in the\n",
    "resulting image. Define what occlusions and disocclusions are in this context and explain\n",
    "how they can lead to visible artifacts in the warped image (e.g., holes, smeared pixels).\n",
    "- Occlusion: Part of source not visible in target.\n",
    "- Disocclusion: Part of target not visible in source.They cause missing pixels or artifacts in the warped image.\n",
    "\n"
   ],
   "id": "6f96cff89b198724"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# VIII Self Supervised learning:",
   "id": "c53a436cdb75a3d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Question 1:\n",
    "\n",
    "- Softmax with τ = 0.1 → sharper distribution (more confident).\n",
    "\n",
    "- Softmax with τ = 10.0 → flatter distribution (less confident)."
   ],
   "id": "b2217cc83af1a2ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Question 2 :\n",
    "- Why use low τ in contrastive learning?\n",
    "- It makes the model focus on the most similar (positive) sample, leading to stronger gradients and better feature separation.\n",
    "\n"
   ],
   "id": "b0957f7ea4a4772f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Question 3\n",
    "- NT-Xent Loss:\n",
    "\n",
    "- ```L(i, j) = -log [ exp(sim(z_i, z_j)/τ) / sum_{k ≠ i} exp(sim(z_i, z_k)/τ) ]```\n",
    "\n",
    "- Numerator: similarity of positive pair.\n",
    "\n",
    "- Denominator: similarities to all other (negative) samples.\n",
    "\n",
    "- τ: controls sharpness/confidence.\n",
    "\n"
   ],
   "id": "5eeb6f9fd20b8c41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Question 4 (What is the \"pretext task\" in the SimCLR framework? Describe the two key steps involved in\n",
    "creating the data for this task and what the model is trained to do.)\n",
    "- Pretext task in SimCLR:\n",
    "\n",
    "- Create two augmentations of the same image.\n",
    "\n",
    "- Train model to bring them closer in feature space than other (different) images.\n",
    "\n",
    "\n"
   ],
   "id": "d110b2ad487f83f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Question 5\n",
    "\n",
    "- t-SNE: Visualizes learned embeddings. Good = clusters by class.\n",
    "\n",
    "- Linear Probing: Train a linear classifier on frozen embeddings. High accuracy → strong representations.\n",
    "\n"
   ],
   "id": "61d3cda1a66368dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Question 6:\n",
    "- Linear Probing:\n",
    "- Freeze backbone, train only a linear classifier on top. Used to evaluate the quality of learned embeddings.\n",
    "\n"
   ],
   "id": "eeb7e09db20b9549"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Question 7: (How does the training objective of CLIP (Contrastive Language-Image Pretraining) differ\n",
    "from that of SimCLR? What are the \"positive pairs\" and \"negative pairs\" in the context of\n",
    "CLIP's training?)\n",
    " -  SimCLR matches views of the same image. \n",
    "  - CLIP matches images and text. Positive = correct image-caption pair. Negative = mismatched pairs."
   ],
   "id": "76e7afae370e804a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "- Question 8: \n",
    "- Zero-shot inference with CLIP:\n",
    "\n",
    "- Encode input image using CLIP image encoder.\n",
    "\n",
    "- Encode class names using CLIP text encoder.\n",
    "\n",
    "- Compute similarity between image embedding and each text embedding.\n",
    "\n",
    "- Predict class with highest similarity."
   ],
   "id": "4985512dc3bb4a21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Question 9:\n",
    "\n",
    "- What is zero-shot learning?\n",
    "- Predicting unseen classes without retraining.\n",
    "\n",
    "- Why can CLIP do this but ResNet can’t?\n",
    "- CLIP learns joint image-text space. It generalizes via text prompts, while traditional classifiers only recognize seen labels."
   ],
   "id": "c1d4e665b2a1442d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5f18fe085c8d36a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
