{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# I- Intro to pytorch and Convolution\n",
   "id": "d089cdee645f5889"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:01:50.323602Z",
     "start_time": "2025-07-18T12:01:46.124584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import PIL.Image as I\n",
    "import torch\n",
    "from scipy.fftpack import fft2, ifft2,ifftshift, fftshift  \n",
    "import cv2"
   ],
   "id": "8d72b95b6dc365f7",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 1 (Pseudo-Code)\n",
    "Write a pseudo-code snippet that performs the following steps:\n",
    "1. Assume you are given a NumPy array named color image with a shape of (H, W, C) where H=512, W=512, and C=3.\n",
    "2. Assume you are also given a NumPy array named gray_patch with a shape of (P, P) where P=64. \n",
    "3. Insert the gray_patch into the top-left corner of the color image. Since the patch is grayscale, its values should be replicated across all three color channels of the target region in color_image."
   ],
   "id": "68b9a003ee822d36"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-18T19:49:30.025818Z",
     "start_time": "2025-07-18T19:49:29.900770Z"
    }
   },
   "source": [
    "color_image = I.open(\"/Users/ryanqchiqache/PycharmProjects/Machine-Learning-Learning-Center/ComputerVisionCourse/Exercise02/saturn.png\").convert(\"RGB\")\n",
    "gray_patch_np = np.resize(color_image, (128,128))\n",
    "gray_patch = np.transpose(gray_patch_np, (2,2))\n",
    "replicated_patch = np.stack([gray_patch] * 3, axis=-1)\n",
    "color_image[0:64, 0:64, :] = replicated_patch"
   ],
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 2 is out of bounds for array of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAxisError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[24]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m color_image = I.open(\u001B[33m\"\u001B[39m\u001B[33m/Users/ryanqchiqache/PycharmProjects/Machine-Learning-Learning-Center/ComputerVisionCourse/Exercise02/saturn.png\u001B[39m\u001B[33m\"\u001B[39m).convert(\u001B[33m\"\u001B[39m\u001B[33mRGB\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      2\u001B[39m gray_patch_np = np.resize(color_image, (\u001B[32m128\u001B[39m,\u001B[32m128\u001B[39m))\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m gray_patch = \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgray_patch_np\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m replicated_patch = np.stack([gray_patch] * \u001B[32m3\u001B[39m, axis=-\u001B[32m1\u001B[39m)\n\u001B[32m      5\u001B[39m color_image[\u001B[32m0\u001B[39m:\u001B[32m64\u001B[39m, \u001B[32m0\u001B[39m:\u001B[32m64\u001B[39m, :] = replicated_patch\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/myproject-env-py3118/lib/python3.11/site-packages/numpy/core/fromnumeric.py:655\u001B[39m, in \u001B[36mtranspose\u001B[39m\u001B[34m(a, axes)\u001B[39m\n\u001B[32m    588\u001B[39m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_transpose_dispatcher)\n\u001B[32m    589\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtranspose\u001B[39m(a, axes=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    590\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    591\u001B[39m \u001B[33;03m    Returns an array with axes transposed.\u001B[39;00m\n\u001B[32m    592\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    653\u001B[39m \n\u001B[32m    654\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m655\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mtranspose\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/myproject-env-py3118/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59\u001B[39m, in \u001B[36m_wrapfunc\u001B[39m\u001B[34m(obj, method, *args, **kwds)\u001B[39m\n\u001B[32m     56\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, *args, **kwds)\n\u001B[32m     58\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m59\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbound\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     60\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[32m     61\u001B[39m     \u001B[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001B[39;00m\n\u001B[32m     62\u001B[39m     \u001B[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     66\u001B[39m     \u001B[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001B[39;00m\n\u001B[32m     67\u001B[39m     \u001B[38;5;66;03m# exception has a traceback chain.\u001B[39;00m\n\u001B[32m     68\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, *args, **kwds)\n",
      "\u001B[31mAxisError\u001B[39m: axis 2 is out of bounds for array of dimension 2"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 2 (Free-Form)\n",
    "- In image processing pipelines, it is common to convert image data between different data\n",
    "types and ranges. \n",
    "- Explain why you would normalize an 8-bit image array (with pixel values\n",
    "in the range [0, 255]) to a floating-point array (with values in the range [0, 1]) before applying\n",
    "a filter like a Gaussian blur."
   ],
   "id": "248b1ddbbdc8fb11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": " # NOTE We normalize 8-bit images to [0, 1] floats before applying Gaussian blur to ensure numerical precision, prevent overflow, and match the floating-point expectations of the filter.",
   "id": "6a18d3aec310e7ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 3 (Pseudo-Code)\n",
    " - Most deep learning frameworks like PyTorch expect image data in the format [Batch,\n",
    "Channels, Height, Width] (B, C, H, W)\n",
    " - while libraries like Pillow and Matplotlib often work with [Height, Width, Channels] (H, W, C). \n",
    "  - Write a pseudo-code function convert_hwc_to_bchw(image_array) that takes a single image as a NumPy array in HWC format and converts it into a PyTorch tensor suitable for a model, with a batch size of 1."
   ],
   "id": "33aec0d8000f55de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T19:57:25.187910Z",
     "start_time": "2025-07-18T19:57:25.183324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_hwc_bchw(image_array):\n",
    "    tensor = torch.from_numpy(image_array).permute(2,0,1)\n",
    "    bchw = tensor.unsqueeze(0)\n",
    "    return bchw.to(dtype=torch.float32)\n",
    "\n",
    "np_array = np.random.randint(0, 255, (128, 128, 3), dtype=np.uint8)\n",
    "\n",
    "print(convert_hwc_bchw(np_array).shape)"
   ],
   "id": "f1eff608b2763e97",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 4 (Free-Form)\n",
    "- Explain the two primary differences between a NumPy ndarray and a PyTorch Tensor that make Tensors more suitable for deep learning."
   ],
   "id": "cce54d09f06c9fce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# NOTE PyTorch tensors are better suited for deep learning because they support GPU acceleration and automatic differentiation for backpropagation.",
   "id": "76249441c17a22df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 5 (Pseudo-Code)\n",
    "- Implement a function convolve_2d(image, kernel) from scratch using basic array operations (e.g., loops and element-wise multiplication). \n",
    "- The function should perform a 2D cross-correlation (as is standard in CNNs).\n",
    "- Inputs: A 2D single-channel image (H x W NumPy array) and a 2D kernel (K x K NumPy array).\n",
    "- Output: A 2D filtered image.\n",
    "- Assumptions: Use a stride of 1 and no padding. The output image size will be smaller than the input."
   ],
   "id": "4a32490b1de6d798"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T20:04:06.632666Z",
     "start_time": "2025-07-18T20:04:06.621988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def conv_2d(img: I, kernel: np.ndarray, stride=1, bias=-1):\n",
    "    H, W = img.shape\n",
    "    kH, kW = kernel.shape\n",
    "    \n",
    "    out_h = H - kH // stride +1\n",
    "    out_w = W - kW // stride +1\n",
    "    \n",
    "    output = np.zeros((out_h, out_w))\n",
    "    \n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            region = img[i*stride:i*stride+kH, j*stride: j*stride+kW]\n",
    "            output[i, j] = np.sum(region * kernel) +bias\n",
    "            \n",
    "    return output\n",
    "           \n",
    "# Example\n",
    "\n",
    "img = np.array([[1, 2, 3],\n",
    "                [4, 5, 6],\n",
    "                [7, 8, 9]])\n",
    "\n",
    "kernel = np.array([[1, 0],\n",
    "                   [0, -1]])\n",
    "\n",
    "out = conv_2d(img, kernel, stride=1, bias=0)\n",
    "print(out)\n"
   ],
   "id": "9e2d40bbf6362c13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4. -4.]\n",
      " [-4. -4.]]\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 6 (Free-Form)\n",
    "\n",
    "The following formula defines a 2D Gaussian filter kernel:\n",
    "\n",
    "*f(x, y) = (1 / (2πσ²)) * exp(-(x² + y²) / (2σ²))*\n",
    "\n",
    "1. **What is the primary effect of applying a Gaussian filter to an image?**  \n",
    "   →  \"It smooths the image by averaging pixel intensities with a weighted kernel, reducing noise and fine detail.\"\n",
    "\n",
    "2. **How does changing the value of σ (sigma) affect this outcome?**  \n",
    "   → \" A larger σ results in more blurring, as the kernel becomes wider and includes more neighboring pixels in the averaging process. A smaller σ keeps more detail.\""
   ],
   "id": "6135091cb9b89c25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 7 (Free-Form)\n",
    "- A Laplace filter is often used for edge detection. A common 3x3 kernel for the Laplacian operator is:\n",
    "- Generated code:\n",
    "[[ 0, 1, 0],\n",
    "[ 1,-4, 1],\n",
    "[ 0, 1, 0]]\n",
    "- Explain briefly, in terms of what the filter calculates, why this kernel highlights edges and regions of rapid intensity change in an image. \n",
    "- (Hint: Think about what the filter approximates mathematically)."
   ],
   "id": "1f2db4128016fb69"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T20:04:47.402184Z",
     "start_time": "2025-07-18T20:04:47.399401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NOTE This kernel highlights edges because it approximates the **second spatial derivative** (the Laplacian) of the image. \n",
    "# NOTE It emphasizes regions where the intensity changes rapidly — i.e., edges — by subtracting the central pixel's value relative to its neighbors."
   ],
   "id": "ea3a777f31df7228",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f2a58e423613046"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 8 (Free-Form)\n",
    "\n",
    "**Two major limitations of hand-crafted filters:**\n",
    "\n",
    "1. They are **fixed and manually designed**, so they can’t adapt to complex patterns or data variability.\n",
    "2. They lack **learnability**, meaning they can’t improve over time or extract higher-level features.\n",
    "\n",
    "**CNNs overcome these by:**\n",
    "- Learning optimal filters during training through backpropagation.\n",
    "- Stacking multiple layers to extract hierarchical features (edges → textures → objects).\n"
   ],
   "id": "f2038a60fc8ce114"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " ### Question 9 (Free-Form)\n",
    "\n",
    "- conv_layer = torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2)\n",
    "1.  Explain the purpose of the in_channel and out_channel: \n",
    "     - in_channels=3: Specifies the number of channels in the input image (e.g., RGB).\n",
    "     - out_channels=32: Specifies the number of filters (feature maps) the layer will learn, resulting in 32 output channels.\n",
    "\n",
    "2. What is the shape of the learnable weight tensor in conv_layer :\n",
    "     - conv_layer.weight.shape → [32, 3, 5, 5] where 32 is the output channels (filters), 3 input channels per filter, and 5x5 kernel size"
   ],
   "id": "a94af8b4fdbf9e8f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 10 (Free-Form)\n",
    "- An input tensor with Input shape:[16, 3, 128, 128]  # (Batch, Channels, Height, Width) is passed to the con_layer of Question 9. Calculate the shape of the output tensor.\n",
    "-  out_dim = ((input_dim = 128 + 2 * padding = 2 - kernel_size = 5) / stride = 1 + ) 1 = ((128 + 4 - 5) / 1 ) + 1 =  128\n",
    "-  out_shape = [16, 32, 128, 128]"
   ],
   "id": "1570f9403214e370"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 11 (Free-Form)\n",
    "- implementation in NumPy can be identical to the output of PyTorch's nn.Conv2d layer (when using the same kernel).\n",
    " - What does this imply about the fundamental operation that nn.Conv2d performs? \n",
    "  - Why is using the PyTorch layer vastly more powerful and efficient in a deep learning context?\n",
    "#### Answer :\n",
    "- If a custom NumPy convolution matches the output of nn.Conv2d (with the same kernel), it means that nn.Conv2d performs the same fundamental operation — a cross-correlation over the input tensor.\n",
    "- However, PyTorch's Conv2d is far more powerful because:\n",
    "    - It supports GPU acceleration for large-scale, efficient training.\n",
    "    - It integrates with PyTorch's autograd system, enabling automatic differentiation and backpropagation."
   ],
   "id": "39da9615d0ef25c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# II-  Fourier Transformation",
   "id": "fe0c7c99f3932e06"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 1\n",
    "- Q: The Discrete Fourier Transform (DFT) of an image produces a complex-valued result. What are the two components called, and what does each represent in terms of the image's frequency content?\n",
    "\n",
    "- A: The two components are the magnitude spectrum, representing the strength of frequencies, and the phase spectrum, representing the spatial arrangement of those frequencies in the image."
   ],
   "id": "929ff98fc3049615"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Quesiton 2\n",
    "- Q: What is the purpose of fftshift when visualizing the 2D Fourier transform of an image? Where is the zero-frequency (DC) component located before and after applying fftshift?\n",
    "\n",
    "- A: fftshift moves the DC component from the top-left corner (default position) to the center of the frequency spectrum, making visualization of frequency content more intuitive."
   ],
   "id": "d62387cd8c1e0761"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 3\n",
    " - Q: What would the centered Fourier magnitude spectrum look like for Image A (horizontal stripes) and Image B (vertical stripes)\n",
    " - A: Image A will show strong vertical frequency components (bright vertical lines), while Image B will show strong horizontal frequency components after rotation."
   ],
   "id": "2c8d1c57b211520a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 4\n",
    "- Q: How are the magnitude and phase spectra affected by the following transformations?\n",
    "\n",
    "- Answers : \n",
    "- Rotating the image by 30 degrees: rotates both magnitude and phase spectra.\n",
    "- Flipping horizontally: mirrors the phase spectrum; magnitude remains symmetric.\n",
    "- Increasing contrast: scales the magnitude spectrum; phase remains unchanged."
   ],
   "id": "e5df0e00eab9f23f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 5\n",
    "- Q: Write a pseudo-code function fourier_denoise(image, threshold) that denoises an image by keeping only the strongest frequency components."
   ],
   "id": "9699a162b2330bd9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T20:25:18.566580Z",
     "start_time": "2025-07-18T20:25:18.556409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fourier_denoise(image, threshold):\n",
    "    F = fft2(image)\n",
    "    F_shifted = fftshift(F)\n",
    "    \n",
    "    magnitude = np.abs(F_shifted) > threshold\n",
    "    \n",
    "    F_filtered = F_shifted * magnitude\n",
    "    F_ishifted = ifftshift(F_filtered)\n",
    "    denoised = ifft2(F_ishifted)\n",
    "    \n",
    "    return np.real(denoised)\n",
    "    "
   ],
   "id": "c1bc8f51fba7bf53",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 6\n",
    "- Q: What is the difference between a low-pass filter and a high-pass filter in the Fourier domain? and what is it used for ?\n",
    "\n",
    "- A: A low-pass filter preserves low frequencies (center of the spectrum) and is used for blurring or denoising. A high-pass filter preserves high frequencies (edges of the spectrum) and is used for edge detection."
   ],
   "id": "2c65f8c4bd575058"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 7\n",
    "- Q: What is aliasing in image down-sampling, and what does the Nyquist-Shannon theorem say about avoiding it?\n",
    "- A: Aliasing occurs when high frequencies are misrepresented as low ones. To avoid it, the sampling rate must be at least twice the highest frequency present in the image."
   ],
   "id": "3e84cc943a94389e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 8\n",
    "- Q: What essential pre-processing step is required before down-sampling an image?\n",
    "- A: Low-pass filtering is needed to remove high-frequency content that would cause aliasing when the image is resized."
   ],
   "id": "80067c144bbf89c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 9\n",
    "- Q: Describe the three-step procedure to resize an image while minimizing aliasing.\n",
    "- A :"
   ],
   "id": "961bc1e3dfc0a4b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T20:30:02.963017Z",
     "start_time": "2025-07-18T20:30:02.955039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def low_pass_filter(image):\n",
    "    # Apply a Gaussian blur or other smoothing filter\n",
    "    kernel = cv2.getGaussianKernel(ksize=5, sigma=1)\n",
    "    gaussian = kernel @ kernel.T\n",
    "    return cv2.filter2D(image, -1, gaussian)\n",
    "\n",
    "def resize_nearest(image, scale):\n",
    "    # Resize using nearest-neighbor interpolation\n",
    "    new_size = (int(image.shape[1] * scale), int(image.shape[0] * scale))\n",
    "    return cv2.resize(image, new_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "def resize_with_antialiasing(image, scale):\n",
    "    # Step 1: Blur the image to remove high frequencies\n",
    "    blurred = low_pass_filter(image)\n",
    "    \n",
    "    # Step 2: Resize using nearest neighbor (safe from aliasing)\n",
    "    resized = resize_nearest(blurred, scale)\n",
    "    return resized"
   ],
   "id": "feecad404d4240c0",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 10 (Free-Form)\n",
    "\n",
    "- Q: Why does Gaussian blur before resizing improve quality, and what is this technique called?\n",
    "\n",
    "- A: Gaussian blur removes high frequencies that cause jagged edges when down-sampling. This pre-filtering step is called anti-aliasing and leads to smoother, higher-quality results."
   ],
   "id": "ff9bc6780dbbb85f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# III- Detection and Segmentation",
   "id": "9fdb3aa769bd8264"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 1\n",
    "\n",
    "- Q: What is the primary goal of the Grad-CAM technique? What two key pieces of information does it extract from a CNN, and from which parts of the computation are they obtained?\n",
    "\n",
    "- A: Grad-CAM highlights regions of the input image that are important for a specific prediction. It extracts: (1) feature maps from the forward pass, and (2) gradients of the target class score w.r.t. those feature maps from the backward pass."
   ],
   "id": "8a0014f6efc266bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 2\n",
    "\n",
    "- Q: Why is a late convolutional layer (e.g., layer4 in ResNet) typically used in Grad-CAM instead of an early layer?\n",
    "\n",
    "- A: Late layers capture more semantic information (what the object is) but have lower spatial resolution. Early layers have higher spatial resolution but low-level features (edges, textures). Grad-CAM prefers semantic relevance over pixel precision.\n",
    "\n"
   ],
   "id": "d70eb97e0a950503"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 3\n",
    "- Q: Write a pseudo-code function calculate_grad_cam(activations, gradients) that computes Grad-CAM."
   ],
   "id": "6ed6afc31375700c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:11:05.372019Z",
     "start_time": "2025-07-18T12:11:05.362669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_grad_cam(activations, gradients):\n",
    "    # Step 1: Compute global average pooling of gradients\n",
    "    alpha = gradients.mean(axis=(1, 2))  # shape: (C,)\n",
    "\n",
    "    # Step 2: Weighted sum of the activations\n",
    "    weighted_sum = (alpha[:, None, None] * activations).sum(axis=0)  # shape: (H, W)\n",
    "\n",
    "    # Step 3: Apply ReLU\n",
    "    heatmap = np.maximum(weighted_sum, 0)\n",
    "    return heatmap"
   ],
   "id": "9b923d74190856d4",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 4\n",
    "\n",
    "- Q: What is a \"hook\" in PyTorch, and why is it used in Grad-CAM?\n",
    "\n",
    "- A: A hook is a function that allows you to extract intermediate data from a model during the forward or backward pass. In Grad-CAM:\n",
    "- A forward hook captures feature maps.\n",
    "- A backward hook captures gradients w.r.t. those maps.\n",
    "\n"
   ],
   "id": "d1c4381515427ec3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 5\n",
    "- Q: What role do the final FC layer weights play in CAM, and what kind of architecture does this require?\n",
    "\n",
    "- A: CAM uses the FC weights to compute a weighted sum over the final convolutional feature maps. This requires a network with a Global Average Pooling (GAP) layer before the FC layer, as in the original CAM paper."
   ],
   "id": "e3f41a6e0d58ae78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 6\n",
    "\n",
    "- Q: Compare Grad-CAM and CAM in terms of how they compute feature importance. What makes Grad-CAM more general?\n",
    "\n",
    "- A: CAM uses FC weights directly (requires GAP + FC structure). Grad-CAM uses gradients to compute importance weights, making it applicable to any CNN architecture, not just those with GAP layers."
   ],
   "id": "48b8785a210b358c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 7\n",
    "- Q: How do you process a 7x7 class activation map to a binary 224x224 mask?\n",
    "- A: Upscale to input resolution using for example cv2.resize(cam, wanted_shape, linear) then normalize to [0,1] then binarize.astype(np.unit8)"
   ],
   "id": "f279a0cd9db23d9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 8\n",
    "- Q: Why is it important to visualize color-coded, blended segmentation masks?\n",
    "\n",
    "- A: It helps users intuitively understand where and why the model is predicting each class."
   ],
   "id": "7e7526b5ecf0ce5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 9\n",
    "- Q: How does pseudo-segmentation differ from fully-trained segmentation networks like FCNs?\n",
    "- first uses no pixel-level labels during training -> can give low-res masks\n",
    "- second uses ground truth segmentation masks -> gives high resolution segmentations\n"
   ],
   "id": "571236f75d5b4f4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# IV- Convolutional Neural Networks",
   "id": "b7bbc97d14cf3c8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 1 \n",
    "- Do exercise for Convolution calculation in sheet 4"
   ],
   "id": "4d3317a2c3d45e19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 2\n",
    "- Q: Why use stride 2 in CNNs, and what is another layer that reduces spatial dimensions?\n",
    "\n",
    "- A: Stride 2 reduces spatial dimensions to lower computational cost and to extract coarse-level features. Another example is max-pooling, which downsamples by taking the max value in local regions."
   ],
   "id": "ac3566a6237c6cd1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 3\n",
    "- Q: What features might the following 2x2 kernels detect?\n",
    "\n",
    "w1 = [[1, 0], [0, 1]]: Diagonal pattern detection (main diagonal)\n",
    "\n",
    "w2 = [[1, 0], [-1, 0]]: Vertical edge detector (difference across rows)\n",
    "\n",
    "w3 = [[1, 1], [0, 0]]: Horizontal edge detector (emphasis on top rows)"
   ],
   "id": "18d8eadfa88ec65b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 4\n",
    "- Q : Calculate the receptive field of a neuron in the final feature map of a CNN with the following architecture:\n",
    "- Layer 1: Convolution with kernel size 5x5, stride 1.\n",
    "- Layer 2: Max-Pooling with kernel size 2x2, stride 2.\n",
    "- Layer 3: Convolution with kernel size 3x3, stride 1.\n",
    "\n",
    "- A :\n",
    "- "
   ],
   "id": "fbe6ca52a0adb957"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 5\n",
    "- Q: Training accuracy is 100%, test accuracy is 75%. What is this, and how to fix it?\n",
    "\n",
    "- A: This is overfitting — the model memorizes training data. Fix it with:\n",
    "\n",
    "- Regularization (e.g., dropout, weight decay)\n",
    "\n",
    "- Data augmentation to increase variability"
   ],
   "id": "a853101ba9306f77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 6\n",
    "- Q: What’s the purpose of model.train() vs model.eval() in PyTorch?\n",
    "\n",
    "- A: These modes switch behavior:\n",
    "\n",
    "- train(): Enables dropout, batchnorm updates\n",
    "\n",
    "- eval(): Freezes dropout and batchnorm (inference mode)"
   ],
   "id": "4207faaad7e1c1d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 7, 8, 9, 10, 11\n",
    "- Questions :\n",
    "- Q7: Why use data augmentation (e.g., flipping, cropping) in CNN training?\n",
    "- Q8: What does a saliency map represent?\n",
    "- Q9: How do early vs. deep ResNet layer features differ?\n",
    "- Q10: How to reduce a [64, 56, 56] activation map for visualization?\n",
    "- Q11: How would saliency maps for 'cat' and 'dog' differ in the same image?\n",
    "\n",
    "- Answers :\n",
    "- A7: It increases data diversity, improving model generalization and robustness to variations in input images.\n",
    "- A8: It shows which input pixels most influence the output class. High values indicate strong impact on the prediction.\n",
    "- A9: Early layers detect edges, textures; deeper layers detect semantic objects or class-specific features.\n",
    "- A10: Two options: Mean over channels → single-channel heatmap or Select max or specific channel for targeted visualization\n",
    "- A11: Each highlights regions most relevant to its class — the cat map will focus on cat features, the dog map on dog features. This shows the network learns class-specific spatial cues."
   ],
   "id": "fe95ef7e09c5ae90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# V- Sequence Modeling in Vision",
   "id": "c0dca1ebf003856b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Question 1\n",
    " "
   ],
   "id": "8ddc6414fd25070c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "####",
   "id": "6d4e8a7b5c74c04d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
